{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuuh6WUWroUE+UBzZ8l7P8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/UH-Insure-NSA/blob/main/model/trainer_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJclIUQLaagw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl\n",
        "from transformers import EvalPrediction\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from transformers import set_seed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from model.dataset.constantLengthDataset import ConstantLengthDataset\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    logits, labels = eval_pred\n",
        "    loss = torch.nn.functional.cross_entropy(logits, labels, reduction='mean')\n",
        "    perplexity = torch.exp(loss).item()\n",
        "    return {\"eval_perplexity\": perplexity}\n",
        "\n",
        "class PerplexityLogger(TrainerCallback):\n",
        "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
        "        if state.is_local_process_zero:\n",
        "            if 'loss' in logs:\n",
        "                perplexity = torch.exp(torch.tensor(logs['loss'])).item()\n",
        "                wandb.log({\"train_perplexity\": perplexity}, step=state.global_step)\n",
        "\n",
        "    # def on_evaluate(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
        "    #     if state.is_local_process_zero:\n",
        "    #         if 'eval_loss' in logs:\n",
        "    #             perplexity = torch.exp(torch.tensor(logs['eval_loss'])).item()\n",
        "    #             wandb.log({\"eval_perplexity\": perplexity}, step=state.global_step)\n",
        "\n",
        "\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        tweet = item['text']\n",
        "        encoding = tokenizer(tweet, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        labels = input_ids.clone()\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    set_seed(42)\n",
        "    wandb.login(key=\"xxxxxxxx\")\n",
        "    run = wandb.init(project='xxxxxx', job_type=\"training\")\n",
        "\n",
        "    base_model = \"cognitivecomputations/dolphin-2.8-mistral-7b-v02\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        #trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=32,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "        ],\n",
        "        bias=\"none\",\n",
        "        lora_dropout=0.05,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        base_model,\n",
        "        padding_side=\"right\",\n",
        "        add_eos_token=True,\n",
        "        add_bos_token=True\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Load the original dataset\n",
        "    data_types = ['emotion', 'sentiment', 'offensive', 'hate', 'irony']\n",
        "    base_path = 'data/original_data/'\n",
        "\n",
        "    train_data = {data_type: pd.read_json(f'{base_path}{data_type}/train_{data_type}.json', lines=True) for data_type in data_types}\n",
        "    eval_data = {data_type: pd.read_json(f'{base_path}{data_type}/val_{data_type}.json', lines=True) for data_type in data_types}\n",
        "    test_data = {data_type: pd.read_json(f'{base_path}{data_type}/test_{data_type}.json', lines=True) for data_type in data_types}\n",
        "\n",
        "    # Concatenate all the original data\n",
        "    original_data = pd.concat([train_data[dt] for dt in data_types])\n",
        "    eval_data = pd.concat([eval_data[dt] for dt in data_types])\n",
        "    test_data = pd.concat([test_data[dt] for dt in data_types])\n",
        "\n",
        "    all_data = pd.concat([original_data, eval_data, test_data])\n",
        "    all_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    print(f\"Length of all data: {len(all_data)}\")\n",
        "\n",
        "    # Remove all the rows with missing values\n",
        "    all_data = all_data.dropna()\n",
        "    print(f\"Length of all data after removing NaN: {len(all_data)}\")\n",
        "\n",
        "    # Drop duplicates\n",
        "    all_data = all_data.drop_duplicates(subset=['text'], keep='first')\n",
        "    print(f\"Length of all data after removing duplicates: {len(all_data)}\")\n",
        "\n",
        "    # Remove rows with length less than 10\n",
        "    all_data = all_data[all_data['text'].str.len() >= 10]\n",
        "    print(f\"Length of all data after removing rows with length less than 10: {len(all_data)}\")\n",
        "\n",
        "    print(f\"Length of all data after processing: {len(all_data)}\")\n",
        "\n",
        "    train,test = train_test_split(all_data,test_size=0.05,random_state=42)\n",
        "    print(f\"Length of train data: {len(train)}\")\n",
        "    print(f\"Length of test data: {len(test)}\")\n",
        "\n",
        "    train_dataset = EmotionDataset(train)\n",
        "    eval_dataset = EmotionDataset(test)\n",
        "\n",
        "    # Calculate total training steps for one epoch\n",
        "    total_train_steps = (len(train_dataset) // 8) // 4  # per_device_train_batch_size=8, gradient_accumulation_steps=4\n",
        "\n",
        "    training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        optim=\"adamw_bnb_8bit\",\n",
        "        save_steps=500,\n",
        "        logging_steps=10,\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.001,\n",
        "        fp16=True,\n",
        "        max_grad_norm=1.0,\n",
        "        max_steps=total_train_steps,\n",
        "        warmup_ratio=0.05,\n",
        "        group_by_length=True,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        report_to=\"wandb\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_arguments,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        callbacks=[PerplexityLogger()]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(\"models/dolphin-2.8-mistral-7b-v02\")\n",
        "    tokenizer.save_pretrained(\"models/dolphin-2.8-mistral-7b-v02\")\n",
        "\n",
        "    wandb.finish()\n",
        "    model.config.use_cache = True\n",
        "    model.eval()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}