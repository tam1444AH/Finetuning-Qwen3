{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB3CEpfBW8yzuBefzyRzfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/UH-Insure-NSA/blob/main/model/dataset/constantLengthDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h67W1rpmcrjf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import IterableDataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from model.dataset.util import get_fim_token_ids, permute\n",
        "\n",
        "# Create an Iterable dataset that returns constant-length chunks of tokens from a stream of text files.\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "    \"\"\"\n",
        "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n",
        "        Args:\n",
        "            tokenizer (Tokenizer): The processor used for proccessing the data.\n",
        "            dataset (dataset.Dataset): Dataset with text files.\n",
        "            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n",
        "            seq_length (int): Length of token sequences to return.\n",
        "            num_of_sequences (int): Number of token sequences to keep in buffer.\n",
        "            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n",
        "            fim_rate (float): Rate (0.0 to 1.0) that sample will be permuted with FIM.\n",
        "            fim_spm_rate (float): Rate (0.0 to 1.0) of FIM permuations that will use SPM.\n",
        "            seed (int): Seed for random number generator.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        dataset,\n",
        "        infinite=False,\n",
        "        seq_length=1024,\n",
        "        num_of_sequences=1024,\n",
        "        chars_per_token=3.6,\n",
        "        content_field=\"content\",\n",
        "        fim_rate=0.5,\n",
        "        fim_spm_rate=0.5,\n",
        "        seed=0,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.concat_token_id = tokenizer.eos_token_id\n",
        "        self.dataset = dataset\n",
        "        self.seq_length = seq_length\n",
        "        self.infinite = infinite\n",
        "        self.current_size = 0\n",
        "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
        "        self.content_field = content_field\n",
        "        self.fim_rate = fim_rate\n",
        "        self.fim_spm_rate = fim_spm_rate\n",
        "        self.seed = seed\n",
        "\n",
        "        (\n",
        "            self.suffix_tok_id,\n",
        "            self.prefix_tok_id,\n",
        "            self.middle_tok_id,\n",
        "            self.pad_tok_id,\n",
        "        ) = get_fim_token_ids(self.tokenizer)\n",
        "        if not self.suffix_tok_id and self.fim_rate > 0:\n",
        "            print(\"FIM is not supported by tokenizer, disabling FIM\")\n",
        "            self.fim_rate = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        iterator = iter(self.dataset)\n",
        "        more_examples = True\n",
        "        np_rng = np.random.RandomState(seed=self.seed)\n",
        "        while more_examples:\n",
        "            buffer, buffer_len = [], 0\n",
        "            while True:\n",
        "                if buffer_len >= self.max_buffer_size:\n",
        "                    break\n",
        "                try:\n",
        "                    buffer.append(next(iterator)[self.content_field])\n",
        "                    buffer_len += len(buffer[-1])\n",
        "                except StopIteration:\n",
        "                    if self.infinite:\n",
        "                        iterator = iter(self.dataset)\n",
        "                    else:\n",
        "                        more_examples = False\n",
        "                        break\n",
        "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
        "            all_token_ids = []\n",
        "\n",
        "            for tokenized_input in tokenized_inputs:\n",
        "                # optionally do FIM permutations\n",
        "                if self.fim_rate > 0:\n",
        "                    tokenized_input, np_rng = permute(\n",
        "                        tokenized_input,\n",
        "                        np_rng,\n",
        "                        self.suffix_tok_id,\n",
        "                        self.prefix_tok_id,\n",
        "                        self.middle_tok_id,\n",
        "                        self.pad_tok_id,\n",
        "                        fim_rate=self.fim_rate,\n",
        "                        fim_spm_rate=self.fim_spm_rate,\n",
        "                        truncate_or_pad=False,\n",
        "                    )\n",
        "\n",
        "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
        "            examples = []\n",
        "            for i in range(0, len(all_token_ids), self.seq_length):\n",
        "                input_ids = all_token_ids[i : i + self.seq_length]\n",
        "                if len(input_ids) == self.seq_length:\n",
        "                    examples.append(input_ids)\n",
        "            random.shuffle(examples)\n",
        "            for example in examples:\n",
        "                self.current_size += 1\n",
        "                yield {\n",
        "                    \"input_ids\": torch.LongTensor(example),\n",
        "                    \"labels\": torch.LongTensor(example),\n",
        "                }"
      ]
    }
  ]
}