{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/Finetuning-Qwen3/blob/main/notebooks/codeLLMFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
        "import wandb\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "%pip install nbstripout\n",
        "\n",
        "REPO_URL=\"https://github.com/tam1444AH/Finetuning-Qwen3.git\"\n",
        "REPO=\"Finetuning-Qwen3\"\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# If repo exists, update it; otherwise, clone fresh\n",
        "if os.path.exists(REPO):\n",
        "    print(f\"Repo '{REPO}' exists, pulling latest changes...\")\n",
        "    os.chdir(REPO)\n",
        "    !git reset --hard HEAD   # optional: discard local changes\n",
        "    !git pull\n",
        "else:\n",
        "    print(f\"Cloning repo '{REPO}'...\")\n",
        "    !git clone \"$REPO_URL\" \"$REPO\"\n",
        "    os.chdir(REPO)\n",
        "\n",
        "!nbstripout --install\n",
        "!git branch -a\n",
        "\n",
        "\n",
        "# Install dependencies if present\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    !pip install -r requirements.txt\n",
        "if os.path.exists(\"pyproject.toml\"):\n",
        "    !pip install -e ."
      ],
      "metadata": {
        "id": "vhsfjOTsdjHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U huggingface_hub hf_transfer\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "wandb.login(key=WANDB_TOKEN, relogin=True)\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])"
      ],
      "metadata": {
        "id": "kYAEaTXfi5IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "dlU0gNQY_guS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content\")\n",
        "!pip install -q transformers datasets\n",
        "\n",
        "import json, random\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "U2UgisCib_pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft bitsandbytes flash-attn\n",
        "\n",
        "MODEL = \"Qwen/Qwen3-Coder-30B-A3B-Instruct\"\n",
        "DATASET = \"/content/Finetuning-Qwen3/data/all_hybrid.jsonl\"\n",
        "DATA_COLUMN = \"content\"\n",
        "\n",
        "SEQ_LENGTH = 4096\n",
        "\n",
        "# Training arguments\n",
        "MAX_STEPS = 200\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 12\n",
        "GR_ACC_STEPS = 2\n",
        "LR = 2e-6                     # learning_rate\n",
        "WARMUP_RATIO = 0.03\n",
        "WEIGHT_DECAY = 0.05\n",
        "LR_SCHEDULER_TYPE = \"cosine\"  # lr_scheduler_type\n",
        "WEIGHT_DECAY = 0.05  # weight_decay\n",
        "NUM_WARMUP_STEPS = 100  # num_warmup_steps\n",
        "EVAL_FREQ = 25\n",
        "SAVE_FREQ = 50\n",
        "LOG_FREQ = 10\n",
        "OUTPUT_DIR = \"Qwen3-Coder-30b-v4\"  # output_dir\n",
        "BF16 = True  # bf16\n",
        "FP16 = False  # no_fp16\n",
        "\n",
        "# FIM trasformations arguments\n",
        "FIM_RATE = 0.125  # fim_rate\n",
        "FIM_SPM_RATE = 0.5  # fim_spm_rate\n",
        "\n",
        "\n",
        "# LORA\n",
        "LORA_R = 16  # lora_r\n",
        "LORA_ALPHA = 32  # lora_alpha\n",
        "LORA_DROPOUT = 0.05  # lora_dropout\n",
        "LORA_TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "\n",
        "# bitsandbytes config\n",
        "USE_NESTED_QUANT = True  # use_nested_quant\n",
        "BNB_4BIT_COMPUTE_DTYPE = \"bfloat16\"  # bnb_4bit_compute_dtype\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "wandb.init(\n",
        "    project=\"qwen3coder-finetune-fp16\",\n",
        "    name=f\"run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        ")\n",
        "wandb.define_metric(\"train/global_step\")\n",
        "wandb.define_metric(\"train/*\", step_metric=\"train/global_step\")\n",
        "wandb.define_metric(\"eval/*\",  step_metric=\"train/global_step\")\n"
      ],
      "metadata": {
        "id": "_exHfg8sVH3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    set_seed,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainerCallback\n",
        ")"
      ],
      "metadata": {
        "id": "8lRTsgnlVcMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/Finetuning-Qwen3\")\n",
        "from model.util import chars_token_ratio, split_by_filetype\n",
        "os.chdir(\"/content\")"
      ],
      "metadata": {
        "id": "osWBahUUpu_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL,\n",
        "    padding_side=\"right\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files={\"data\": DATASET})[\"data\"]\n",
        "\n",
        "unsupervised = dataset.filter(lambda x: x[\"set\"] == \"unsupervised\")\n",
        "\n",
        "cryptol = unsupervised.filter(lambda x: x[\"filetype\"] == \"cry\")\n",
        "saw = unsupervised.filter(lambda x: x[\"filetype\"] == \"saw\")\n",
        "text = unsupervised.filter(lambda x: x[\"filetype\"] == \"txt\")\n",
        "\n",
        "cryptol_split = cryptol.train_test_split(test_size=0.1, seed=SEED, shuffle=True)\n",
        "saw_split = saw.train_test_split(test_size=0.1, seed=SEED, shuffle=True)\n",
        "text_split = text.train_test_split(test_size=0.1, seed=SEED, shuffle=True)\n",
        "\n",
        "train_ds = concatenate_datasets([cryptol_split[\"train\"], saw_split[\"train\"], text_split[\"train\"]])\n",
        "eval_ds = concatenate_datasets([cryptol_split[\"test\"], saw_split[\"test\"], text_split[\"test\"]])\n",
        "\n",
        "print(train_ds[0].keys())\n",
        "assert DATA_COLUMN in train_ds.column_names, f\"Missing '{DATA_COLUMN}' in JSONL!\"\n",
        "\n",
        "print(eval_ds[0].keys())\n",
        "assert DATA_COLUMN in eval_ds.column_names, f\"Missing '{DATA_COLUMN}' in JSONL!\"\n",
        "\n",
        "print(len(train_ds), len(eval_ds))\n",
        "\n",
        "chars_per_token = chars_token_ratio(train_ds, tokenizer, DATA_COLUMN)\n",
        "print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")"
      ],
      "metadata": {
        "id": "vlcO8zw22Ycd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.util import *"
      ],
      "metadata": {
        "id": "_WIZWSfIJBaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.constantLengthDataset import ConstantLengthDataset\n",
        "import torch\n",
        "\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=train_ds,\n",
        "        infinite=False,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        chars_per_token=chars_per_token,\n",
        "        content_field=DATA_COLUMN,\n",
        "        fim_rate=FIM_RATE,\n",
        "        fim_spm_rate=FIM_SPM_RATE,\n",
        "        overlap_ratio=0.125,\n",
        "        seed=SEED,\n",
        "        already_tokenized=False,\n",
        ")\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=eval_ds,\n",
        "        infinite=False,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        chars_per_token=chars_per_token,\n",
        "        content_field=DATA_COLUMN,\n",
        "        fim_rate=FIM_RATE,\n",
        "        fim_spm_rate=FIM_SPM_RATE,\n",
        "        overlap_ratio=0.125,\n",
        "        seed=SEED,\n",
        "        already_tokenized=False,\n",
        ")"
      ],
      "metadata": {
        "id": "tC8QbrmIJBcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "gc.collect(); torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "KltZZq0QbJFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U bitsandbytes\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft.tuners.lora import LoraLayer\n",
        "\n",
        "names = [\"<|fim_prefix|>\", \"<|fim_middle|>\", \"<|fim_suffix|>\", \"<|fim_pad|>\"]\n",
        "ids = [tokenizer.convert_tokens_to_ids(t) for t in names]\n",
        "print(dict(zip(names, ids)))\n",
        "print(\"additional_special_tokens:\", tokenizer.special_tokens_map.get(\"additional_special_tokens\"))\n",
        "\n",
        "# 4-bit quantization\n",
        "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
        ")\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL,\n",
        "        load_in_8bit=False,\n",
        "        quantization_config=bnb_config,\n",
        "        dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=True,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "base = prepare_model_for_kbit_training(base)"
      ],
      "metadata": {
        "id": "6r7iIuyAJBg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = base.model.layers[0]           # Llama/Qwen-style\n",
        "print(\"ATTN:\", blk.self_attn)         # has q_proj, k_proj, v_proj, o_proj\n",
        "print(\"MLP:\", blk.mlp)\n",
        "target_modules = LORA_TARGET_MODULES"
      ],
      "metadata": {
        "id": "P5monDpDR8_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up lora\n",
        "from peft import PeftModel\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "\n",
        "# model = PeftModel.from_pretrained(base, \"tam2003/peft-FT-3-Coder-30b-v3\", is_trainable=True) # If your just trying to test/train the model.\n",
        "\n",
        "model = get_peft_model(base, peft_config) # If you want to continue training.\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "-RFqsImVJBjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "2bApn9UHJz7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.start_iteration = 0\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=5,\n",
        "    gradient_accumulation_steps=GR_ACC_STEPS,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=LR,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    warmup_steps=26,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=13,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=13,\n",
        "    save_strategy=\"steps\"\n",
        "    save_steps=13,\n",
        "    report_to=[\"wandb\"],\n",
        "    push_to_hub=True,\n",
        "    dataloader_drop_last=False,\n",
        "    dataloader_num_workers=2,\n",
        "    include_tokens_per_second=True,\n",
        "    max_grad_norm=0.3,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    dataloader_pin_memory=True,\n",
        "    save_safetensors=True,\n",
        "    seed=SEED,\n",
        ")"
      ],
      "metadata": {
        "id": "If0nsW1tJNQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from model.util import PerplexityCallback, EpochToDatasetCallback\n",
        "from time import time\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,         # required for early stopping\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    callbacks=[\n",
        "        PerplexityCallback(),\n",
        "        EpochToDatasetCallback(train_dataset),\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=2,\n",
        "            early_stopping_threshold=0.0\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "train_start_time = time()\n",
        "print(\"Training...\")\n",
        "trainer.train(resume_from_checkpoint=False)\n",
        "train_end_time = time()\n",
        "print(f\"Training completed in {train_end_time - train_start_time:.2f} seconds.\")\n",
        "eval_results = trainer.evaluate()\n",
        "wandb.finish()\n",
        "eval_loss = eval_results[\"eval_loss\"]\n",
        "perplexity = math.exp(eval_loss)\n",
        "print(f\"Eval loss = {eval_loss:.2f}, Perplexity = {perplexity:.2f}\")"
      ],
      "metadata": {
        "id": "hrZA8wr0JNSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "I-jKxX1UJdu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "BmmcKA_3y_CL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}