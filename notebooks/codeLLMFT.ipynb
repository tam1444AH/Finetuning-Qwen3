{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/UH-Insure-NSA/blob/main/notebooks/codeLLMFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "%pip install nbstripout\n",
        "\n",
        "REPO_URL=\"https://github.com/tam1444AH/UH-Insure-NSA.git\"\n",
        "REPO=\"UH-Insure-NSA\"\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# If repo exists, update it; otherwise, clone fresh\n",
        "if os.path.exists(REPO):\n",
        "    print(f\"Repo '{REPO}' exists, pulling latest changes...\")\n",
        "    os.chdir(REPO)\n",
        "    !git reset --hard HEAD   # optional: discard local changes\n",
        "    !git pull\n",
        "else:\n",
        "    print(f\"Cloning repo '{REPO}'...\")\n",
        "    !git clone \"$REPO_URL\" \"$REPO\"\n",
        "    os.chdir(REPO)\n",
        "\n",
        "!nbstripout --install\n",
        "!git branch -a\n",
        "\n",
        "\n",
        "# Install dependencies if present\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    !pip install -r requirements.txt\n",
        "if os.path.exists(\"pyproject.toml\"):\n",
        "    !pip install -e .\n",
        "\n",
        "# save test."
      ],
      "metadata": {
        "id": "vhsfjOTsdjHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.test import test\n",
        "test()"
      ],
      "metadata": {
        "id": "Wgetafsaeq2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U huggingface_hub hf_transfer\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])"
      ],
      "metadata": {
        "id": "kYAEaTXfi5IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# project_id = 'code-llm-finetuning'\n",
        "# !gcloud config set project {project_id}\n",
        "\n",
        "# GCS_PATH = \"gs://code-llm-fine-tuning-security-analytics/data/source_code.jsonl\"\n",
        "# LOCAL_PATH = \"/tmp/source_code.jsonl\"\n",
        "\n",
        "# !gsutil ls {GCS_PATH}\n",
        "# !gsutil -m cp {GCS_PATH} {LOCAL_PATH}\n",
        "\n",
        "# !ls -lh {LOCAL_PATH}\n",
        "# !head -n 2 {LOCAL_PATH}\n",
        "\n",
        "# import json\n",
        "\n",
        "# examples = [\n",
        "#     {\"filename\": \"xor.cry\", \"content\": \"module Xor where\\nproperty xor_inverse (a:[8], b:[8]) = ((a ^^ b) ^^ b) == a\"},\n",
        "#     {\"filename\": \"xor.saw\", \"content\": \"cryptol_load \\\"xor.cry\\\";\\nprove_print z3 {{ Xor::xor_inverse }};\"}\n",
        "# ]\n",
        "\n",
        "# with open(\"/tmp/source_code.jsonl\", \"w\") as f:\n",
        "#     for ex in examples:\n",
        "#         f.write(json.dumps(ex) + \"\\n\")\n",
        "\n",
        "# !ls -lh /tmp/source_code.jsonl\n",
        "# !head -n 2 /tmp/source_code.jsonl\n",
        "\n",
        "\n",
        "\n",
        "#!gsutil cp gs://code-llm-fine-tuning-security-analytics/data/source_code.jsonl /tmp/source_code.jsonl\n",
        "#!cat /tmp/source_code.jsonl"
      ],
      "metadata": {
        "id": "dlU0gNQY_guS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content\")\n",
        "# !git clone https://github.com/GaloisInc/cryptol-specs.git\n",
        "# !pip install tqdm\n",
        "\n",
        "# import os, json, re\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# ROOT_DIR = \"cryptol-specs\"\n",
        "# OUT_FILE = \"/tmp/source_code.jsonl\"\n",
        "\n",
        "# # Heuristic filters (you can tweak these)\n",
        "# MIN_LEN = 50          # skip tiny examples\n",
        "# MAX_LEN = 200_000     # skip giant test vectors\n",
        "# MIN_KEYWORDS = [\"module\", \"property\", \"where\"]\n",
        "# EXCLUDE_DIRS = [\"test\", \"deprecated\", \"examples/Regression\"]"
      ],
      "metadata": {
        "id": "U2UgisCib_pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def is_valid_cry(path, content):\n",
        "#     for ex in EXCLUDE_DIRS:\n",
        "#         if ex.lower() in path.lower():\n",
        "#             return False\n",
        "#     if len(content) < MIN_LEN or len(content) > MAX_LEN:\n",
        "#         return False\n",
        "#     if sum(c.isascii() for c in content) / len(content) < 0.8:\n",
        "#         return False\n",
        "#     if not any(kw in content for kw in MIN_KEYWORDS):\n",
        "#         return False\n",
        "#     return True\n",
        "\n",
        "# records = []\n",
        "# for root, _, files in os.walk(ROOT_DIR):\n",
        "#     for fname in files:\n",
        "#         if fname.endswith(\".cry\"):\n",
        "#             fpath = os.path.join(root, fname)\n",
        "#             try:\n",
        "#                 with open(fpath, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "#                     content = f.read()\n",
        "#                 if is_valid_cry(fpath, content):\n",
        "#                     records.append({\n",
        "#                         \"filename\": os.path.relpath(fpath, ROOT_DIR),\n",
        "#                         \"content\": content\n",
        "#                     })\n",
        "#             except Exception as e:\n",
        "#                 print(\"Error reading\", fpath, \":\", e)\n",
        "\n",
        "# with open(OUT_FILE, \"w\", encoding=\"utf-8\") as out:\n",
        "#     for rec in records:\n",
        "#         out.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "# print(f\"Done. Wrote {len(records)} Cryptol specs to {OUT_FILE}\")\n",
        "# !ls -lh {OUT_FILE}\n",
        "# !head -n 3 {OUT_FILE}"
      ],
      "metadata": {
        "id": "ytxyasbneGrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, hashlib\n",
        "\n",
        "src = \"/content/verified_GPThybrid.jsonl\"\n",
        "out = \"/content/hybrid_tagged_unsupervised.jsonl\"\n",
        "\n",
        "def sha1_of(text):\n",
        "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()[:10]\n",
        "\n",
        "with open(out, \"w\") as f_out, open(src, \"r\") as f_in:\n",
        "  for line in f_in:\n",
        "    try:\n",
        "      data = json.loads(line)\n",
        "      filetype = data.get(\"filetype\", \"\").lower()\n",
        "      filename = data.get(\"filename\", \"unknown\")\n",
        "      content = data.get(\"content\", \"\").strip()\n",
        "      variant = data.get(\"variant\", \"hybrid\")\n",
        "\n",
        "      if filetype == \"cry\" or filename.endswith(\".cry\"):\n",
        "          tag = \"cryptol\"\n",
        "      elif filetype == \"saw\" or filename.endswith(\".saw\"):\n",
        "          tag = \"saw\"\n",
        "      else:\n",
        "          tag = \"unknown\"\n",
        "\n",
        "      sha = sha1_of(content)\n",
        "      wrapped = (\n",
        "          f\"<FILE:{tag}> <PATH:{filename}> <VARIANT:{variant}> <SHA:{sha}>\\n\"\n",
        "          f\"<CODE>\\n{content}\\n</CODE>\\n</FILE>\\n\"\n",
        "      )\n",
        "      f_out.write(json.dumps({\"content\": wrapped}) + \"\\n\")\n",
        "    except Exception as e:\n",
        "      print(\"skip:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "udergJl-__oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft bitsandbytes flash-attn\n",
        "\n",
        "MODEL = \"Qwen/Qwen2.5-Coder-32B\"  # Model checkpoint on the Hugging Face Hub\n",
        "DATASET = \"/content/hybrid_tagged_unsupervised.jsonl\" #Josh's preproccesed dataset.\n",
        "DATA_COLUMN = \"content\"  # Column name containing the code content\n",
        "\n",
        "SEQ_LENGTH = 2048  # Sequence length\n",
        "\n",
        "# Training arguments\n",
        "# MAX_STEPS = 1#2000  # max_steps\n",
        "MAX_STEPS = 200#2000  # max_steps\n",
        "BATCH_SIZE = 1  # batch_size\n",
        "# GR_ACC_STEPS = 16  # gradient_accumulation_steps\n",
        "GR_ACC_STEPS = 8  # gradient_accumulation_steps\n",
        "# LR = 5e-4  # learning_rate\n",
        "LR = 2e-4                     # a bit lower for stability\n",
        "LR_SCHEDULER_TYPE = \"cosine\"  # lr_scheduler_type\n",
        "WEIGHT_DECAY = 0.05  # weight_decay\n",
        "# WEIGHT_DECAY = 0.01  # weight_decay\n",
        "NUM_WARMUP_STEPS = 100  # num_warmup_steps\n",
        "# NUM_WARMUP_STEPS = 30  # num_warmup_steps\n",
        "# EVAL_FREQ = 200\n",
        "EVAL_FREQ = 50\n",
        "SAVE_FREQ = 200\n",
        "LOG_FREQ = 10\n",
        "LOG_FREQ = 50\n",
        "# EVAL_FREQ = 100  # eval_freq\n",
        "# SAVE_FREQ = 100  # save_freq\n",
        "# LOG_FREQ = 25  # log_freq\n",
        "OUTPUT_DIR = \"peft-FT-2.5-Coder-32b\"  # output_dir\n",
        "BF16 = True  # bf16\n",
        "FP16 = False  # no_fp16\n",
        "\n",
        "# FIM trasformations arguments\n",
        "FIM_RATE = 0.5  # fim_rate\n",
        "FIM_SPM_RATE = 0.5  # fim_spm_rate\n",
        "\n",
        "# LORA\n",
        "LORA_R = 8  # lora_r\n",
        "LORA_ALPHA = 32  # lora_alpha\n",
        "LORA_DROPOUT = 0.05  # lora_dropout\n",
        "LORA_TARGET_MODULES = \"c_proj,c_attn,q_attn,c_fc,c_proj\"  # lora_target_modules\n",
        "\n",
        "# bitsandbytes config\n",
        "USE_NESTED_QUANT = True  # use_nested_quant\n",
        "BNB_4BIT_COMPUTE_DTYPE = \"bfloat16\"  # bnb_4bit_compute_dtype\n",
        "\n",
        "SEED = 0"
      ],
      "metadata": {
        "id": "_exHfg8sVH3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    set_seed,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "8lRTsgnlVcMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "DATASET = \"/content/hybrid_tagged_unsupervised.jsonl\"\n",
        "DATA_COLUMN = \"content\"\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": DATASET})\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# quick sanity check: make sure the field exists\n",
        "print(train_data[0].keys())\n",
        "assert DATA_COLUMN in train_data.column_names, f\"Missing '{DATA_COLUMN}' in JSONL!\"\n"
      ],
      "metadata": {
        "id": "3cMn_T0mE5cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
        "\n",
        "def chars_token_ratio(dataset, tokenizer, data_column, nb_examples=400):\n",
        "    \"\"\"\n",
        "    Estimate the average number of characters per token in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    total_characters, total_tokens = 0, 0\n",
        "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
        "        total_characters += len(example[data_column])\n",
        "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
        "\n",
        "    return total_characters / total_tokens\n",
        "\n",
        "\n",
        "chars_per_token = chars_token_ratio(train_data, tokenizer, DATA_COLUMN)\n",
        "print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n"
      ],
      "metadata": {
        "id": "vlcO8zw22Ycd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.dataset.util import *"
      ],
      "metadata": {
        "id": "_WIZWSfIJBaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.dataset.constantLengthDataset import ConstantLengthDataset\n",
        "\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        train_data,\n",
        "        infinite=True,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        chars_per_token=chars_per_token,\n",
        "        content_field=DATA_COLUMN,\n",
        "        fim_rate=FIM_RATE,\n",
        "        fim_spm_rate=FIM_SPM_RATE,\n",
        "        seed=SEED,\n",
        ")\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        train_data,\n",
        "        #valid_data,\n",
        "        infinite=False,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        chars_per_token=chars_per_token,\n",
        "        content_field=DATA_COLUMN,\n",
        "        fim_rate=FIM_RATE,\n",
        "        fim_spm_rate=FIM_SPM_RATE,\n",
        "        seed=SEED,\n",
        ")"
      ],
      "metadata": {
        "id": "tC8QbrmIJBcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft.tuners.lora import LoraLayer\n",
        "\n",
        "load_in_8bit = False\n",
        "\n",
        "# 4-bit quantization\n",
        "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
        ")\n",
        "\n",
        "device_map = \"auto\"#{\"\": 0}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=device_map,\n",
        "        use_cache=False,  # We will be using gradient checkpointing\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "        offload_folder=\"/content/offload\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "6r7iIuyAJBg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = model.model.layers[0]           # Llama/Qwen-style\n",
        "print(\"ATTN:\", blk.self_attn)         # has q_proj, k_proj, v_proj, o_proj\n",
        "print(\"MLP:\", blk.mlp)\n",
        "target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]"
      ],
      "metadata": {
        "id": "P5monDpDR8_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up lora\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "-RFqsImVJBjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.start_iteration = 0\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"tam1444AH/{OUTPUT_DIR}\",\n",
        "    dataloader_drop_last=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    max_steps=MAX_STEPS,\n",
        "    eval_steps=EVAL_FREQ,\n",
        "    save_steps=SAVE_FREQ,\n",
        "    logging_steps=LOG_FREQ,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    warmup_steps=NUM_WARMUP_STEPS,\n",
        "    gradient_accumulation_steps=GR_ACC_STEPS,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=FP16,\n",
        "    bf16=BF16,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    push_to_hub=True,\n",
        "    include_tokens_per_second=True,\n",
        "    report_to=[]\n",
        ")\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Sample text:\\n\\n{train_data[0][DATA_COLUMN][:400]}\")"
      ],
      "metadata": {
        "id": "If0nsW1tJNQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "print(\"Training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "hrZA8wr0JNSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "I-jKxX1UJdu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers peft\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\", trust_remote_code=True)\n",
        "\n",
        "# Load your LoRA fine-tuned adapter on top of it\n",
        "model = PeftModel.from_pretrained(model, \"tam2003/peft-FT-2.5-Coder-32b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\", trust_remote_code=True)\n",
        "\n",
        "# Quick test\n",
        "# prompt = \"Write a Cryptol specification for a function xor_inverse that proves (a XOR b) XOR b == a\"\n",
        "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "# outputs = model.generate(**inputs, max_new_tokens=150)\n",
        "\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "Nluh80FiGggK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import gc, torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"balanced_low_0\",  # spreads layers more evenly\n",
        "    dtype=\"bfloat16\",        # uses half precision\n",
        ")\n",
        "\n",
        "prompt = \"\"\"\n",
        "Translate this English spec into Cryptol:\n",
        "\n",
        "\"A function sha256_hash that takes a 512-bit input block and returns a 256-bit digest.\"\n",
        "\"\"\"\n",
        "result = pipe(prompt, max_new_tokens=300)\n",
        "print(result[0][\"generated_text\"])\n",
        "\n",
        "prompts = [\n",
        "    'Translate this English spec into Cryptol:\\n\\n\"A function xor_inverse that proves (a XOR b) XOR b == a\"',\n",
        "    'Translate this English spec into Cryptol:\\n\\n\"A function sha256_hash that takes a 512-bit input and returns a 256-bit digest\"',\n",
        "    'Translate this English spec into Cryptol:\\n\\n\"Create a property proving that addition is commutative for 8-bit words.\"'\n",
        "]\n",
        "\n",
        "for p in prompts:\n",
        "    print(\"\\n=== Prompt ===\")\n",
        "    print(p)\n",
        "    inputs = tokenizer(p, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
        "    print(\"\\n--- Generated Cryptol ---\")\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "bl7xpvXyzn_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -q\n",
        "!apt-get install -y wget tar\n",
        "\n",
        "!wget -q https://github.com/GaloisInc/saw-script/releases/download/v1.3/saw-1.3-ubuntu-22.04-X64-with-solvers.tar.gz\n",
        "!tar -xzf saw-1.3-ubuntu-22.04-X64-with-solvers.tar.gz\n",
        "!mv saw-1.3-ubuntu-22.04-X64-with-solvers saw\n",
        "\n",
        "!wget -q https://github.com/GaloisInc/cryptol/releases/download/3.3.0/cryptol-3.3.0-ubuntu-20.04-X64-with-solvers.tar.gz\n",
        "!tar -xzf cryptol-3.3.0-ubuntu-22.04-X64-with-solvers.tar.gz\n",
        "!mv cryptol-3.3.0-ubuntu-22.04-X64-with-solvers cryptol"
      ],
      "metadata": {
        "id": "4hY45O6PXIrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PATH\"] = \"/content/saw/bin:/content/cryptol/bin:\" + os.environ[\"PATH\"]"
      ],
      "metadata": {
        "id": "wl8RGxi-XKht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!saw --version\n",
        "!cryptol --version"
      ],
      "metadata": {
        "id": "RfgWMHyLXMVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/GaloisInc/cryptol-specs.git"
      ],
      "metadata": {
        "id": "0AB_seQTXNQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Verifications\n",
        "os.environ[\"CRYPTOLPATH\"] = \"/content/cryptol-specs\""
      ],
      "metadata": {
        "id": "WFkYonnTXO1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!saw AES128.saw"
      ],
      "metadata": {
        "id": "BB4iNM7QXQt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/plobethus/crypto-c.git"
      ],
      "metadata": {
        "id": "xqNHAwlpYYOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd crypto-c"
      ],
      "metadata": {
        "id": "iqbNBsGoYwjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x scripts/colab_setup.sh\n",
        "!bash scripts/colab_setup.sh\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "ftXjFoP7ZcCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}