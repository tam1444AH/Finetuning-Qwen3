{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMTfcVD9NHC9a76Vsxj2Wdw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/Finetuning-Qwen3/blob/main/notebooks/supervised-data-preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"huggingface-hub<1.0\" accelerate peft bitsandbytes\n",
        "!pip -q install -U ms-swift transformers\n"
      ],
      "metadata": {
        "id": "-3Bk5TE_p1nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, transformers, huggingface_hub, sys, os\n",
        "os.environ[\"PATH\"] += \":/root/.local/bin:/usr/local/bin\"\n",
        "print(\"swift bin:\", shutil.which(\"swift\"))\n",
        "print(\"ms-swift bin:\", shutil.which(\"ms-swift\"))\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"huggingface_hub:\", huggingface_hub.__version__)\n"
      ],
      "metadata": {
        "id": "a8mov5Caq0Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install -U huggingface_hub hf_transfer\n",
        "!pip -q install -U hf_transfer\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "import wandb\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "wandb.login(key=WANDB_TOKEN, relogin=True)\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])"
      ],
      "metadata": {
        "id": "Cp0_l1uxh36L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, sys\n",
        "\n",
        "THINK = \"<think>\\n\\n</think>\\n\\n\"\n",
        "\n",
        "def to_messages(row):\n",
        "  instruction = row.get(\"instruction\",\"\").strip()\n",
        "  input = row.get(\"input\",\"\").strip()\n",
        "  output = row.get(\"output\",\"\").strip()\n",
        "\n",
        "\n",
        "  user = instruction if not input else f\"{instruction}\\n\\n[Input]\\n{input}\"\n",
        "\n",
        "  assistant = THINK + output\n",
        "\n",
        "  sft_row = {\"messages\":[{\"role\":\"user\",\"content\":user},\n",
        "                         {\"role\":\"assistant\",\"content\":assistant}]}\n",
        "\n",
        "  return sft_row\n"
      ],
      "metadata": {
        "id": "sgaLZ_rYVDcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_sample_data = open(\"/content/sft_sample_data.jsonl\", \"w\")\n",
        "with open(\"/content/alpaca_instruct_sft_samples2.jsonl\") as fin:\n",
        "  for line in fin:\n",
        "    if not line.strip(): continue\n",
        "\n",
        "    row = json.loads(line)\n",
        "    msg = to_messages(row)\n",
        "\n",
        "    sft_sample_data.write(json.dumps(msg) + \"\\n\")\n",
        "\n",
        "sft_sample_data.close()"
      ],
      "metadata": {
        "id": "UpPlOpMSYkcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$DATA\" \"$OUT\"\n",
        "DATA=\"${1:-sft_sample_data.jsonl}\"\n",
        "OUT=\"${2:-sft_sample_data_out}\"\n",
        "\n",
        "export SWIFT_USE_HF=1\n",
        "\n",
        "swift sft \\\n",
        "  --use_hf true \\\n",
        "  --model Qwen/Qwen2.5-Coder-32B \\\n",
        "  --adapters tam2003/peft-FT-2.5-Coder-32b \\\n",
        "  --dataset \"$DATA\" \\\n",
        "  --output_dir \"$OUT\" \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --warmup_ratio 0.03 \\\n",
        "  --per_device_train_batch_size 2 \\\n",
        "  --gradient_accumulation_steps 8 \\\n",
        "  --torch_dtype float16 \\\n",
        "  --gradient_checkpointing true \\\n",
        "  --lora_r 16 \\\n",
        "  --lora_alpha 32 \\\n",
        "  --lora_dropout 0.05 \\\n",
        "  --save_steps 200 \\\n",
        "  --logging_steps 20 \\\n",
        "  --loss_scale ignore_empty_think\n"
      ],
      "metadata": {
        "id": "RiSemxr2jWbR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}