{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQdDxgmjf/wGGxSxmtSXJy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/UH-Insure-NSA/blob/main/notebooks/Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "emJ1PjEM3Y-n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"output/file_metrics.csv\")\n",
        "\n",
        "# Remove a local directory from the \"filename\" column\n",
        "df[\"filename\"] = df[\"filename\"].str.replace(\n",
        "    \"/Users/josh/SecurityAnalytics/development/\", \"\", regex=False\n",
        ")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "XAi33gXz4Iiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bytes_cutoff = df['bytes'].quantile(0.95)\n",
        "lines_cutoff = df['lines'].quantile(0.95)\n",
        "\n",
        "df_filtered = df[(df['bytes'] <= bytes_cutoff) & (df['lines'] <= lines_cutoff)]\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Violin plot for bytes\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(y=df_filtered['bytes'], color=\"steelblue\", inner=\"quartile\")\n",
        "plt.title(\"Violin Plot of File Sizes (bytes, outliers removed)\")\n",
        "plt.ylabel(\"Bytes\")\n",
        "\n",
        "# Violin plot for lines\n",
        "plt.subplot(1,2,2)\n",
        "sns.violinplot(y=df_filtered['lines'], color=\"darkorange\", inner=\"quartile\")\n",
        "plt.title(\"Violin Plot of File Sizes (lines, outliers removed)\")\n",
        "plt.ylabel(\"Lines\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-i_u4Ssa4M3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier clipping thresholds\n",
        "lines_cut = df['lines'].quantile(0.95)\n",
        "avg_cut   = df['avg_line_len'].quantile(0.95)\n",
        "max_cut   = df['max_line_len'].quantile(0.95)\n",
        "\n",
        "df_filtered = df[\n",
        "    (df['lines'] <= lines_cut) &\n",
        "    (df['avg_line_len'] <= avg_cut) &\n",
        "    (df['max_line_len'] <= max_cut)\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "# Violin plot for lines\n",
        "plt.subplot(1,3,1)\n",
        "sns.violinplot(y=df_filtered['lines'], color=\"skyblue\", inner=\"quartile\")\n",
        "plt.title(\"Violin Plot: Lines (95 percentile)\")\n",
        "\n",
        "# Violin plot for avg_line_len\n",
        "plt.subplot(1,3,2)\n",
        "sns.violinplot(y=df_filtered['avg_line_len'], color=\"lightgreen\", inner=\"quartile\")\n",
        "plt.title(\"Violin Plot: Avg Line Length (95 percentile)\")\n",
        "\n",
        "# Violin plot for max_line_len\n",
        "plt.subplot(1,3,3)\n",
        "sns.violinplot(y=df_filtered['max_line_len'], color=\"salmon\", inner=\"quartile\")\n",
        "plt.title(\"Violin Plot: Max Line Length (95 percentile)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A-iEQ4ZW4Ocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "drop_columns = ['filename','sha1','num_tokens_model', 'binary_like', 'enc_hits_unicode', 'k_shingle']\n",
        "corr = df.drop(columns=drop_columns).corr(numeric_only=True)\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", annot=True, fmt=\".2f\", annot_kws={\"size\": 8}, center=0)\n",
        "plt.title(\"Correlation Heatmap of Metrics\")\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q38vtPHz4Qfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "HWzu2HjE4S3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# StarCoder-like thresholds (tune if needed)\n",
        "MAX_BYTES         = 200_000\n",
        "MAX_NONASCII      = 0.20\n",
        "ENC_MAX_RUN_CHARS = 1024\n",
        "ENC_MAX_FRACTION  = 0.50\n",
        "MAX_LINES_TOTAL   = 100_000\n",
        "MAX_LINE_AVG_LEN  = 100\n",
        "MAX_LINE_MAX_LEN  = 1_000\n",
        "MIN_TOKENS_LANG   = 40      # language-token gate (Cryptol tokenizer)\n",
        "MAX_TOKENS_LANG   = 10_000  # optional upper bound\n",
        "MIN_TOKENS_MODEL  = 32      # only if youâ€™ve populated num_tokens_model\n",
        "MAX_HEXNUM_RATIO  = 0.20"
      ],
      "metadata": {
        "id": "hLo0bG3T4TxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- exact dedup (keep first occurrence of each sha1) ---\n",
        "# mark duplicates (True means \"is duplicate\" => drop later)\n",
        "dup_mask = df.duplicated(subset=[\"sha1\"], keep=\"first\")\n",
        "\n",
        "# --- encoded data (StarCoder) ---\n",
        "enc_mask = (df[\"enc_max_run\"] > ENC_MAX_RUN_CHARS) | (df[\"enc_fraction\"] > ENC_MAX_FRACTION)\n",
        "\n",
        "# --- long-line filters (StarCoder) ---\n",
        "longline_mask = (\n",
        "    (df[\"lines\"] > MAX_LINES_TOTAL) |\n",
        "    (df[\"avg_line_len\"] > MAX_LINE_AVG_LEN) |\n",
        "    (df[\"max_line_len\"] > MAX_LINE_MAX_LEN)\n",
        ")\n",
        "\n",
        "# --- binary-like content ---\n",
        "binary_mask = df[\"binary_like\"].fillna(False)\n",
        "\n",
        "# --- non-ascii density ---\n",
        "nonascii_mask = df[\"non_ascii_ratio\"].fillna(0) > MAX_NONASCII\n",
        "\n",
        "# --- size guardrail (bytes) ---\n",
        "bytes_mask = df[\"bytes\"].fillna(0) > MAX_BYTES\n",
        "\n",
        "# --- language-token bounds ---\n",
        "lang_small_mask = df[\"num_tokens_lang\"].fillna(0) < MIN_TOKENS_LANG\n",
        "lang_large_mask = df[\"num_tokens_lang\"].fillna(0) > MAX_TOKENS_LANG\n",
        "\n",
        "# --- shingles exist (needed for Jaccard) ---\n",
        "no_shingles_mask = df[\"num_shingles\"].fillna(0) <= 0\n",
        "\n",
        "# --- numeric/hex blob concentration ---\n",
        "hexnum_mask = df[\"hexnum_ratio\"].fillna(0) > MAX_HEXNUM_RATIO\n",
        "\n",
        "# --- model-token gate (only apply where available) ---\n",
        "if \"num_tokens_model\" in df.columns:\n",
        "    model_small_mask = df[\"num_tokens_model\"].fillna(np.inf) < MIN_TOKENS_MODEL\n",
        "else:\n",
        "    model_small_mask = pd.Series(False, index=df.index)"
      ],
      "metadata": {
        "id": "WD4kq_kT4YDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all hard-drop reasons\n",
        "drop_mask = (\n",
        "    dup_mask |\n",
        "    enc_mask |\n",
        "    longline_mask |\n",
        "    binary_mask |\n",
        "    nonascii_mask |\n",
        "    bytes_mask |\n",
        "    lang_small_mask |\n",
        "    lang_large_mask |\n",
        "    no_shingles_mask |\n",
        "    hexnum_mask |\n",
        "    model_small_mask\n",
        ")\n",
        "\n",
        "# Optional: compute a human-readable fail reason (first rule that tripped)\n",
        "def first_reason(i):\n",
        "    if dup_mask.iat[i]:          return \"exact_duplicate\"\n",
        "    if enc_mask.iat[i]:          return \"encoded_data\"\n",
        "    if longline_mask.iat[i]:     return \"long_lines\"\n",
        "    if binary_mask.iat[i]:       return \"binary_like\"\n",
        "    if nonascii_mask.iat[i]:     return \"too_much_nonascii\"\n",
        "    if bytes_mask.iat[i]:        return \"too_large_bytes\"\n",
        "    if lang_small_mask.iat[i]:   return \"too_few_lang_tokens\"\n",
        "    if lang_large_mask.iat[i]:   return \"too_many_lang_tokens\"\n",
        "    if no_shingles_mask.iat[i]:  return \"no_shingles\"\n",
        "    if hexnum_mask.iat[i]:       return \"hexnum_blob\"\n",
        "    if model_small_mask.iat[i]:  return \"too_few_model_tokens\"\n",
        "    return \"ok\"\n",
        "\n",
        "df = df.copy()\n",
        "df[\"quality_ok\"] = ~drop_mask\n",
        "df[\"fail_reason\"] = [first_reason(i) for i in range(len(df))]"
      ],
      "metadata": {
        "id": "Wf5v1vwk4Yon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dedup_cols = [\n",
        "    \"filename\", \"sha1\",\n",
        "    # size/lines\n",
        "    \"bytes\", \"lines\", \"avg_line_len\", \"max_line_len\",\n",
        "    # content/encoding\n",
        "    \"non_ascii_ratio\", \"binary_like\",\n",
        "    \"enc_total_matched\", \"enc_max_run\", \"enc_fraction\",\n",
        "    \"enc_hits_base64\", \"enc_hits_hexbytes\", \"enc_hits_unicode\",\n",
        "    # tokens/shingles\n",
        "    \"num_tokens_lang\", \"k_shingle\", \"num_shingles\", \"hexnum_ratio\",\n",
        "    # model tokens (optional)\n",
        "    \"num_tokens_model\",\n",
        "    # path heuristic & status\n",
        "    \"junk_path\", \"quality_ok\", \"fail_reason\",\n",
        "]\n",
        "\n",
        "candidate_df = df.loc[df[\"quality_ok\"], dedup_cols].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "k_8yxkyZ4af3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[summary] total:\", len(df))\n",
        "print(\"[summary] kept :\", int(df[\"quality_ok\"].sum()))\n",
        "print(\"[summary] dropped:\", int((~df[\"quality_ok\"]).sum()))\n",
        "print(\"[summary] drop reasons:\")\n",
        "print(df.loc[~df[\"quality_ok\"], \"fail_reason\"].value_counts())"
      ],
      "metadata": {
        "id": "qm3Ll4J74ct2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_df.head()"
      ],
      "metadata": {
        "id": "UwZjZw4O4fql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing.similiar_process import run_from_dataframe\n",
        "\n",
        "# candidate_df must have an absolute-path 'filename' column.\n",
        "df_files, df_pairs, similar_files = run_from_dataframe(\n",
        "    candidate_df,\n",
        "    filename_col=\"filename\",\n",
        "    root_dir=\"/Users/josh/SecurityAnalytics/development\",  # prepended to filename when opening\n",
        "    out_dir=\"minhash_outputs\",\n",
        ")"
      ],
      "metadata": {
        "id": "mSrOXjqJ4gyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_files.head()"
      ],
      "metadata": {
        "id": "t-ZnwYk34kD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pairs.head()"
      ],
      "metadata": {
        "id": "vI8Kl77E4on-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_files"
      ],
      "metadata": {
        "id": "QfZO6Hdz4qi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing.cluster_process import run_clustering\n",
        "\n",
        "# If you already have df_files/df_pairs in memory:\n",
        "df_keep, df_drop, df_clusters = run_clustering(\n",
        "    df_files=df_files,          # from similiar_process\n",
        "    df_pairs=df_pairs,          # from similiar_process\n",
        "    jaccard_keep_threshold=0.70,\n",
        "    out_dir=\"minhash_outputs\",\n",
        "    content_lookup=None,        # or {filename: raw_text} if you want text-derived penalties\n",
        "    save_outputs=True\n",
        ")"
      ],
      "metadata": {
        "id": "oni4jQ3f4rRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing.dataset_builder import build_datasets_from_sources\n",
        "\n",
        "results = build_datasets_from_sources(\n",
        "    metrics_csv=\"minhash_outputs/dedup_keep.csv\",  # or minhash_files.csv\n",
        "    filename_col=\"filename\",\n",
        "    root_dir=\"/Users/josh/SecurityAnalytics/development\",  # prepended to relative filenames\n",
        "    out_dir=\"out_datasets\",\n",
        "    variants=\"with_comments,without_comments,hybrid\",\n",
        "\n",
        "    # Agent robustness\n",
        "    agent_batch_size=8,                # smaller batches help on tough files\n",
        "    agent_timeout_s=45,                # fail fast if a batch hangs\n",
        "    max_comment_len=4000,\n",
        "    decision_cache_path=\"out_datasets/comment_decisions_cache.jsonl\",\n",
        "\n",
        "    # Qwen2.5-Coder-7B (4096 ctx) with prompt reserve\n",
        "    context_window_tokens=4096,\n",
        "    prompt_reserve_tokens=600,         # adjust to your FT prompt template\n",
        "    chunk_overlap_tokens=64,\n",
        "    chars_per_token=4.0,               # conservative heuristic\n",
        "\n",
        "    # Progress cadence\n",
        "    file_progress_every=20,\n",
        "    save_parquet=True,\n",
        ")"
      ],
      "metadata": {
        "id": "XjR8s2A_4ytd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}