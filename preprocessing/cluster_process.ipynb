{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjU5w64Vc1zFUpvT0/ZS91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/UH-Insure-NSA/blob/main/preprocessing/cluster_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h67W1rpmcrjf"
      },
      "outputs": [],
      "source": [
        "# cluster.py (notebook-friendly)\n",
        "# pip install pandas numpy\n",
        "import os\n",
        "import re\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "# -------- defaults (can be overridden via run_clustering args) --------\n",
        "DEFAULT_JACCARD_KEEP_THRESHOLD = 0.70\n",
        "DEFAULT_OUT_DIR = \"minhash_outputs\"\n",
        "\n",
        "# --- helper: build clusters (connected components) via BFS ---\n",
        "def build_clusters(df_pairs: pd.DataFrame, threshold: float) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Build connected components using edges with jaccard >= threshold.\n",
        "    Returns a list of components; each component is a list of filenames.\n",
        "    \"\"\"\n",
        "    adj = defaultdict(set)\n",
        "    for _, row in df_pairs.iterrows():\n",
        "        if row[\"jaccard\"] >= threshold:\n",
        "            a, b = row[\"a\"], row[\"b\"]\n",
        "            adj[a].add(b)\n",
        "            adj[b].add(a)\n",
        "\n",
        "    nodes = set(df_pairs[\"a\"]).union(set(df_pairs[\"b\"]))\n",
        "    return list(_connected_components(adj, nodes))\n",
        "\n",
        "def _connected_components(adj: Dict[str, set], nodes: Iterable[str]):\n",
        "    seen = set()\n",
        "    for n in nodes:\n",
        "        if n in seen:\n",
        "            continue\n",
        "        comp = []\n",
        "        q = deque([n])\n",
        "        seen.add(n)\n",
        "        while q:\n",
        "            u = q.popleft()\n",
        "            comp.append(u)\n",
        "            for v in adj[u]:\n",
        "                if v not in seen:\n",
        "                    seen.add(v)\n",
        "                    q.append(v)\n",
        "        yield comp\n",
        "\n",
        "# --- quality scoring helpers (unchanged, just wrapped) ---\n",
        "_junk_path_re = re.compile(\n",
        "    r\"(?:^|/)(?:test|tests|example|examples|tmp|backup|backups|copy|copies|vendor|third_party|old)(?:/|$)\",\n",
        "    re.I,\n",
        ")\n",
        "\n",
        "def hex_num_ratio_from_tokens(num_tokens, text):\n",
        "    if not text:\n",
        "        return 0.0\n",
        "    long_hex = len(re.findall(r\"0x[0-9A-Fa-f]{4,}\", text))\n",
        "    long_nums = len(re.findall(r\"\\b\\d{6,}\\b\", text))\n",
        "    denom = max(1, int(num_tokens) if pd.notna(num_tokens) else 1)\n",
        "    return min(1.0, (long_hex + long_nums) / denom)\n",
        "\n",
        "def non_ascii_ratio(text):\n",
        "    if not text:\n",
        "        return 0.0\n",
        "    non = sum(1 for ch in text if ord(ch) > 127)\n",
        "    return non / len(text) if text else 0.0\n",
        "\n",
        "def sweet_spot_score(n, low=50, high=5000):\n",
        "    try:\n",
        "        n = float(n)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    if n <= 0:\n",
        "        return 0.0\n",
        "    if n < low:\n",
        "        return (n / low) * 0.5\n",
        "    if n > high:\n",
        "        return max(0.0, 1.0 - (np.log1p(n - high) / np.log1p(high)))\n",
        "    return 1.0\n",
        "\n",
        "def file_quality_score(row: pd.Series, raw_text: Optional[str] = None,\n",
        "                       parse_ok: Optional[bool] = None, proof_ok: Optional[bool] = None) -> float:\n",
        "    score = 0.0\n",
        "\n",
        "    if parse_ok is True:\n",
        "        score += 3.0\n",
        "    if proof_ok is True:\n",
        "        score += 3.0\n",
        "\n",
        "    def getf(key, default=0.0):\n",
        "        try:\n",
        "            return float(row[key])\n",
        "        except Exception:\n",
        "            return float(default)\n",
        "\n",
        "    size_tok = sweet_spot_score(getf(\"num_tokens\"))\n",
        "    size_shg = sweet_spot_score(getf(\"num_shingles\"))\n",
        "    score += 3.0 * size_tok\n",
        "    score += 2.0 * size_shg\n",
        "\n",
        "    try:\n",
        "        path_val = row[\"filename\"]\n",
        "    except Exception:\n",
        "        path_val = row.name\n",
        "\n",
        "    if isinstance(path_val, str):\n",
        "        path_norm = path_val.replace(\"\\\\\", \"/\")\n",
        "        if _junk_path_re.search(path_norm):\n",
        "            score -= 2.0\n",
        "\n",
        "    if raw_text is not None:\n",
        "        score -= 2.0 * hex_num_ratio_from_tokens(getf(\"num_tokens\"), raw_text)\n",
        "        score -= 1.0 * non_ascii_ratio(raw_text)\n",
        "\n",
        "    return float(score)\n",
        "\n",
        "# -------- main entry (call this from your notebook) --------\n",
        "def run_clustering(\n",
        "    df_files: Optional[pd.DataFrame] = None,\n",
        "    df_pairs: Optional[pd.DataFrame] = None,\n",
        "    *,\n",
        "    files_csv: str = os.path.join(DEFAULT_OUT_DIR, \"minhash_files.csv\"),\n",
        "    pairs_csv: str = os.path.join(DEFAULT_OUT_DIR, \"minhash_pairs.csv\"),\n",
        "    jaccard_keep_threshold: float = DEFAULT_JACCARD_KEEP_THRESHOLD,\n",
        "    out_dir: str = DEFAULT_OUT_DIR,\n",
        "    content_lookup: Optional[Dict[str, str]] = None,  # optional raw text for penalties\n",
        "    save_outputs: bool = True,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Cluster near-duplicate files using Jaccard pairs, select a representative per cluster,\n",
        "    and (optionally) write dedup CSVs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_files : DataFrame with at least [\"filename\",\"num_tokens\",\"num_shingles\"]\n",
        "               If None, loads from files_csv.\n",
        "    df_pairs : DataFrame with at least [\"a\",\"b\",\"jaccard\"]\n",
        "               If None, loads from pairs_csv.\n",
        "    files_csv, pairs_csv : Paths to load if df_* not supplied.\n",
        "    jaccard_keep_threshold : Edge threshold to connect files in a cluster.\n",
        "    out_dir : Where to save outputs if save_outputs=True.\n",
        "    content_lookup : Optional dict {filename -> raw_text} used in scoring.\n",
        "    save_outputs : If True, writes dedup_keep.csv, dedup_drop.csv, dedup_clusters.csv\n",
        "                   and train_files_after_dedup.csv to out_dir.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (df_keep, df_drop, df_clusters)\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if df_files is None:\n",
        "        df_files = pd.read_csv(files_csv)\n",
        "    if df_pairs is None:\n",
        "        df_pairs = pd.read_csv(pairs_csv)\n",
        "\n",
        "    # Ensure every file appears at least as a singleton cluster\n",
        "    all_paths = set(df_files[\"filename\"])\n",
        "    pair_nodes = set(df_pairs[\"a\"]).union(set(df_pairs[\"b\"]))\n",
        "    clusters = build_clusters(df_pairs, jaccard_keep_threshold)\n",
        "\n",
        "    singleton_paths = sorted(all_paths - pair_nodes)\n",
        "    for p in singleton_paths:\n",
        "        clusters.append([p])\n",
        "\n",
        "    if content_lookup is None:\n",
        "        content_lookup = {}\n",
        "\n",
        "    file_index = df_files.set_index(\"filename\")\n",
        "    keep_rows, drop_rows, cluster_rows = [], [], []\n",
        "\n",
        "    for cid, comp in enumerate(clusters, start=1):\n",
        "        scored = []\n",
        "        for p in comp:\n",
        "            if p not in file_index.index:\n",
        "                continue\n",
        "            row = file_index.loc[p]\n",
        "            raw = content_lookup.get(p)  # None if unknown\n",
        "            s = file_quality_score(row, raw_text=raw)\n",
        "            scored.append((p, s, row.get(\"num_shingles\", 0), row.get(\"num_tokens\", 0), len(p)))\n",
        "\n",
        "        if not scored:\n",
        "            continue\n",
        "\n",
        "        scored.sort(key=lambda t: (-t[1], -t[2], -t[3], t[4], t[0]))\n",
        "        keep = scored[0][0]\n",
        "        drops = [p for p, *_ in scored[1:]]\n",
        "\n",
        "        keep_rows.append({\"cluster_id\": cid, \"filename\": keep, \"score\": float(scored[0][1])})\n",
        "        for p, s, ns, nt, _ in scored[1:]:\n",
        "            drop_rows.append({\"cluster_id\": cid, \"filename\": p, \"score\": float(s)})\n",
        "\n",
        "        cluster_rows.append({\n",
        "            \"cluster_id\": cid,\n",
        "            \"size\": len(comp),\n",
        "            \"kept\": keep,\n",
        "            \"dropped_count\": len(drops),\n",
        "        })\n",
        "\n",
        "    df_keep  = pd.DataFrame(keep_rows).sort_values([\"cluster_id\", \"filename\"]).reset_index(drop=True)\n",
        "    df_drop  = pd.DataFrame(drop_rows).sort_values([\"cluster_id\", \"filename\"]).reset_index(drop=True)\n",
        "    df_clust = pd.DataFrame(cluster_rows).sort_values(\"cluster_id\").reset_index(drop=True)\n",
        "\n",
        "    print(f\"[info] clusters formed   : {len(clusters)}\")\n",
        "    print(f\"[info] kept files        : {len(df_keep)}\")\n",
        "    print(f\"[info] dropped files     : {len(df_drop)}\")\n",
        "\n",
        "    if save_outputs:\n",
        "        df_keep.to_csv(os.path.join(out_dir, \"dedup_keep.csv\"), index=False)\n",
        "        df_drop.to_csv(os.path.join(out_dir, \"dedup_drop.csv\"), index=False)\n",
        "        df_clust.to_csv(os.path.join(out_dir, \"dedup_clusters.csv\"), index=False)\n",
        "\n",
        "        keep_set = set(df_keep[\"filename\"])\n",
        "        df_train_files = df_files[df_files[\"filename\"].isin(keep_set)].copy()\n",
        "        df_train_files.to_csv(os.path.join(out_dir, \"train_files_after_dedup.csv\"), index=False)\n",
        "        print(f\"[info] wrote keep/drop/cluster CSVs to {out_dir}/\")\n",
        "\n",
        "    return df_keep, df_drop, df_clust\n",
        "\n",
        "# ----- optional CLI for script use (safe in notebook; only runs if __main__) -----\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    p = argparse.ArgumentParser(description=\"Cluster similar files from MinHash pairs and select representatives.\")\n",
        "    p.add_argument(\"--files-csv\", default=os.path.join(DEFAULT_OUT_DIR, \"minhash_files.csv\"))\n",
        "    p.add_argument(\"--pairs-csv\", default=os.path.join(DEFAULT_OUT_DIR, \"minhash_pairs.csv\"))\n",
        "    p.add_argument(\"--threshold\", type=float, default=DEFAULT_JACCARD_KEEP_THRESHOLD)\n",
        "    p.add_argument(\"--out-dir\", default=DEFAULT_OUT_DIR)\n",
        "    p.add_argument(\"--no-save\", action=\"store_true\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    run_clustering(\n",
        "        df_files=None,\n",
        "        df_pairs=None,\n",
        "        files_csv=args.files_csv,\n",
        "        pairs_csv=args.pairs_csv,\n",
        "        jaccard_keep_threshold=args.threshold,\n",
        "        out_dir=args.out_dir,\n",
        "        save_outputs=not args.no_save,\n",
        "    )"
      ]
    }
  ]
}