{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt6b1X6NxypFYAfD8EdamJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/UH-Insure-NSA/blob/main/preprocessing/quality_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h67W1rpmcrjf"
      },
      "outputs": [],
      "source": [
        "# ---------- metrics helpers ----------\n",
        "import re, hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "# Encoded-data (StarCoder-style)\n",
        "_ENC_BASE64_RE   = re.compile(r\"[A-Za-z0-9+/=\\n]{64,}\")\n",
        "_ENC_HEXBYTES_RE = re.compile(r\"(?:\\b(?:0x|\\\\x)?[0-9A-Fa-f]{2}(?:,|\\b\\s*)){8,}\")\n",
        "_ENC_UNICODE_RE  = re.compile(r\"(?:\\\\u[0-9A-Fa-f]{4}){8,}\")\n",
        "\n",
        "# Binary-like detector (control chars except \\t \\n \\r)\n",
        "_BINARY_CHUNK = re.compile(rb\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\")\n",
        "\n",
        "# Long hex / long number\n",
        "_HEX_LONG_RE = re.compile(r\"0x[0-9A-Fa-f]{4,}\")\n",
        "_NUM_LONG_RE = re.compile(r\"\\b\\d{6,}\\b\")\n",
        "\n",
        "# Junk path\n",
        "_JUNK_PATH_RE = re.compile(\n",
        "    r\"(?:^|/)(?:test|tests|example|examples|tmp|backup|backups|copy|copies|vendor|third_party|old)(?:/|$)\",\n",
        "    re.I\n",
        ")\n",
        "\n",
        "def normalize_newlines(s: str) -> str:\n",
        "    return s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "def sha1_text(s: str) -> str:\n",
        "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def looks_binary(text: str) -> bool:\n",
        "    raw = text.encode(\"utf-8\", errors=\"ignore\")\n",
        "    return bool(_BINARY_CHUNK.search(raw))\n",
        "\n",
        "def line_stats(text: str) -> tuple[int, float, int]:\n",
        "    t = normalize_newlines(text)\n",
        "    lines = t.split(\"\\n\")\n",
        "    if len(lines) == 1 and lines[0] == \"\":\n",
        "        return 0, 0.0, 0\n",
        "    total = 0\n",
        "    maxlen = 0\n",
        "    for ln in lines:\n",
        "        L = len(ln)\n",
        "        total += L\n",
        "        if L > maxlen: maxlen = L\n",
        "    return len(lines), (total / len(lines)), maxlen\n",
        "\n",
        "def non_ascii_ratio(text: str) -> float:\n",
        "    if not text: return 0.0\n",
        "    non = sum(1 for ch in text if ord(ch) > 127)\n",
        "    return non / len(text)\n",
        "\n",
        "def encoded_data_metrics(text: str) -> dict:\n",
        "    length = max(1, len(text))\n",
        "    total_matched = 0\n",
        "    max_run = 0\n",
        "    hits = {\"base64\": 0, \"hexbytes\": 0, \"unicode\": 0}\n",
        "    for name, pat in ((\"base64\", _ENC_BASE64_RE),\n",
        "                      (\"hexbytes\", _ENC_HEXBYTES_RE),\n",
        "                      (\"unicode\", _ENC_UNICODE_RE)):\n",
        "        for m in pat.finditer(text):\n",
        "            span = m.end() - m.start()\n",
        "            total_matched += span\n",
        "            if span > max_run: max_run = span\n",
        "            hits[name] += 1\n",
        "    return {\n",
        "        \"enc_total_matched\": total_matched,\n",
        "        \"enc_max_run\": max_run,\n",
        "        \"enc_fraction\": total_matched / length,\n",
        "        \"enc_hits_base64\": hits[\"base64\"],\n",
        "        \"enc_hits_hexbytes\": hits[\"hexbytes\"],\n",
        "        \"enc_hits_unicode\": hits[\"unicode\"],\n",
        "    }\n",
        "\n",
        "def hex_num_ratio(text: str, token_count_hint: int | None = None) -> float:\n",
        "    long_hex = len(_HEX_LONG_RE.findall(text))\n",
        "    long_num = len(_NUM_LONG_RE.findall(text))\n",
        "    denom = max(1, token_count_hint or 0)\n",
        "    return min(1.0, (long_hex + long_num) / denom)\n",
        "\n",
        "# Default very-light Cryptol-ish tokenizer (you can pass your own)\n",
        "_DEFAULT_TOKENIZER = re.compile(r\"\"\"\n",
        "    0x[0-9A-Fa-f]+ | 0b[01]+ | 0o[0-7]+ | \\d+ |\n",
        "    \"(?:[^\"\\\\]|\\\\.)*\" |\n",
        "    [A-Za-z_][A-Za-z_0-9']* |\n",
        "    <<<|>>>|<<|>>|\\^\\^|::|->|==|<=|>=|!=|<-|\\.\\.|\\.\\.\\.|<\\||\\|> |\n",
        "    [{}()\\[\\];,:\\.@!#\\^+\\-*/=<>\\|`]\n",
        "\"\"\", re.X)\n",
        "\n",
        "def default_tokenize(code: str) -> list[str]:\n",
        "    # strip line/block comments first\n",
        "    code = re.sub(r'--.*?$|//.*?$|/\\*.*?\\*/', '', code, flags=re.S|re.M)\n",
        "    return _DEFAULT_TOKENIZER.findall(code)\n",
        "\n",
        "# ---------- main metrics function ----------\n",
        "def compute_file_metrics(\n",
        "    filename: str,\n",
        "    text: str,\n",
        "    *,\n",
        "    k_shingle: int = 5,\n",
        "    lang_tokenize = None,              # callable: (str)->list[str]\n",
        "    model_tokenizer = None             # HuggingFace tokenizer or any object with encode(add_special_tokens=False)\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Return a flat dict of per-file metrics suitable for a pandas DataFrame row.\n",
        "    Does not make keep/drop decisionsâ€”just measures.\n",
        "    \"\"\"\n",
        "    lang_tokenize = lang_tokenize or default_tokenize\n",
        "\n",
        "    # Normalize once for stable hashing & line stats\n",
        "    norm = normalize_newlines(text)\n",
        "    sha1 = sha1_text(norm)\n",
        "\n",
        "    # Bytes (encoded) and line stats\n",
        "    n_bytes = len(norm.encode(\"utf-8\", errors=\"ignore\"))\n",
        "    n_lines, avg_line_len, max_line_len = line_stats(norm)\n",
        "\n",
        "    # Quick checks\n",
        "    is_binary = looks_binary(norm)\n",
        "    na_ratio  = non_ascii_ratio(norm)\n",
        "\n",
        "    # Encoded data coverage\n",
        "    enc = encoded_data_metrics(norm)\n",
        "\n",
        "    # Language tokens & shingles\n",
        "    toks = lang_tokenize(norm)\n",
        "    num_tokens_lang = len(toks)\n",
        "    num_shingles    = max(0, num_tokens_lang - k_shingle + 1)\n",
        "\n",
        "    # Hex/long-number concentration (per token)\n",
        "    hexnum_ratio = hex_num_ratio(norm, token_count_hint=num_tokens_lang)\n",
        "\n",
        "    # Model tokens (optional)\n",
        "    num_tokens_model = None\n",
        "    if model_tokenizer is not None:\n",
        "        try:\n",
        "            num_tokens_model = len(model_tokenizer.encode(norm, add_special_tokens=False))\n",
        "        except Exception:\n",
        "            num_tokens_model = None\n",
        "\n",
        "    # Junk path heuristic\n",
        "    path_norm = filename.replace(\"\\\\\", \"/\")\n",
        "    junk_path = bool(_JUNK_PATH_RE.search(path_norm))\n",
        "\n",
        "    return {\n",
        "        # identity\n",
        "        \"filename\": filename,\n",
        "        \"sha1\": sha1,\n",
        "\n",
        "        # size / lines\n",
        "        \"bytes\": n_bytes,\n",
        "        \"lines\": n_lines,\n",
        "        \"avg_line_len\": round(avg_line_len, 2),\n",
        "        \"max_line_len\": max_line_len,\n",
        "\n",
        "        # content character stats\n",
        "        \"non_ascii_ratio\": round(na_ratio, 6),\n",
        "        \"binary_like\": is_binary,\n",
        "\n",
        "        # encoded-data coverage\n",
        "        **enc,  # enc_total_matched, enc_max_run, enc_fraction, enc_hits_*\n",
        "\n",
        "        # token/shingle stats\n",
        "        \"num_tokens_lang\": num_tokens_lang,\n",
        "        \"k_shingle\": k_shingle,\n",
        "        \"num_shingles\": num_shingles,\n",
        "        \"hexnum_ratio\": round(hexnum_ratio, 6),\n",
        "\n",
        "        # model tokens (optional)\n",
        "        \"num_tokens_model\": num_tokens_model,\n",
        "\n",
        "        # path heuristics\n",
        "        \"junk_path\": junk_path,\n",
        "    }\n"
      ]
    }
  ]
}