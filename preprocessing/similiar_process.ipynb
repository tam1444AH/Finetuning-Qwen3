{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEmSPxi5Anr2+FsVxIfLeW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/UH-Insure-NSA/blob/main/preprocessing/similiar_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h67W1rpmcrjf"
      },
      "outputs": [],
      "source": [
        "# similiar_process.py\n",
        "# MinHash/LSH similarity over a candidate_df of absolute file paths.\n",
        "# Saves CSV/Parquet/JSONL outputs and returns (df_files, df_pairs, similar_files).\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "from typing import Dict, Tuple, Iterable, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "# ---------- defaults (configurable via run_from_dataframe args) ----------\n",
        "DEFAULT_NUM_PERM      = 512\n",
        "DEFAULT_K_SHINGLE     = 5\n",
        "DEFAULT_LSH_THRESHOLD = 0.70\n",
        "DEFAULT_TOP_N_PRINT   = 20\n",
        "DEFAULT_OUT_DIR       = \"minhash_outputs\"\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def tokenize(code: str) -> List[str]:\n",
        "    # strip line & block comments\n",
        "    code = re.sub(r'--.*?$|//.*?$|/\\*.*?\\*/', '', code, flags=re.S | re.M)\n",
        "\n",
        "    token_pat = re.compile(r\"\"\"\n",
        "        # ---------- literals ----------\n",
        "        0x[0-9A-Fa-f]+            |   # hex\n",
        "        0b[01]+                   |   # binary\n",
        "        0o[0-7]+                  |   # octal\n",
        "        \\d+                       |   # decimal\n",
        "        \"(?:[^\"\\\\]|\\\\.)*\"        |   # strings (basic escapes)\n",
        "\n",
        "        # ---------- identifiers ----------\n",
        "        [A-Za-z_][A-Za-z_0-9']*   |   # allow prime in names (e.g., SubByte')\n",
        "\n",
        "        # ---------- multi-char operators (order matters: longest first) ----------\n",
        "        <<< | >>> | << | >>       |   # shifts/rotates\n",
        "        \\^\\^                      |   # polynomial/exponent\n",
        "        ::  | ->  | == | <= | >=  |   # comparisons/arrows\n",
        "        !=  | <-  | \\.\\. | \\.\\.\\. |   # not-equal, generator, ranges\n",
        "        <\\| | \\|>                  |  # polynomial delimiters\n",
        "\n",
        "        # ---------- single-char punctuation / operators ----------\n",
        "        [{}()\\[\\];,:\\.@!#\\^+\\-*/=<>\\|`]\n",
        "    \"\"\", re.X)\n",
        "\n",
        "    toks = token_pat.findall(code)\n",
        "    return toks\n",
        "\n",
        "def shingles(tokens: List[str], k: int = 5) -> set:\n",
        "    n = len(tokens)\n",
        "    if n < k:\n",
        "        return set()\n",
        "    return {\" \".join(tokens[i:i+k]) for i in range(n - k + 1)}\n",
        "\n",
        "def jaccard(a: set, b: set) -> float:\n",
        "    u = len(a | b)\n",
        "    return (len(a & b) / u) if u else 0.0\n",
        "\n",
        "def to_minhash(sigset: Iterable[str], num_perm: int) -> MinHash:\n",
        "    m = MinHash(num_perm=num_perm)\n",
        "    for s in sigset:\n",
        "        m.update(s.encode(\"utf-8\"))\n",
        "    return m\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _read_text_utf8_normalized(path: str) -> Optional[str]:\n",
        "    \"\"\"Open source file as UTF-8 (errors='replace') and normalize newlines to '\\n'.\"\"\"\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "            t = f.read()\n",
        "        # normalize newlines\n",
        "        t = t.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "        return t\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] cannot read '{path}': {e}\")\n",
        "        return None\n",
        "\n",
        "def _load_corpus_from_df(\n",
        "    df: pd.DataFrame,\n",
        "    filename_col: str = \"filename\",\n",
        "    content_col: Optional[str] = None,\n",
        "    root_dir: Optional[str] = None,\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Build {filename -> content} from a candidate_df.\n",
        "    If content_col provided, use it.\n",
        "    Otherwise open files by joining root_dir (if given) + filename_col.\n",
        "    \"\"\"\n",
        "    if filename_col not in df.columns:\n",
        "        raise KeyError(f\"'{filename_col}' column not found in DataFrame\")\n",
        "\n",
        "    corpus: Dict[str, str] = {}\n",
        "    for i, row in df.iterrows():\n",
        "        relname = row[filename_col]\n",
        "        if not isinstance(relname, str) or not relname:\n",
        "            print(f\"[warn] row {i}: empty or non-string filename; skipping\")\n",
        "            continue\n",
        "\n",
        "        text: Optional[str] = None\n",
        "        if content_col and content_col in df.columns:\n",
        "            raw = row[content_col]\n",
        "            if isinstance(raw, str) and raw:\n",
        "                text = raw.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "        if text is None:\n",
        "            path = os.path.join(root_dir, relname) if root_dir else relname\n",
        "            text = _read_text_utf8_normalized(path)\n",
        "\n",
        "        if text is None:\n",
        "            print(f\"[warn] row {i}: no content for '{relname}'; skipping\")\n",
        "            continue\n",
        "\n",
        "        # KEY: keep relname as filename key (unchanged!)\n",
        "        corpus[relname] = text\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "def _ensure_outdir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# ---------- main entry ----------\n",
        "def run_from_dataframe(\n",
        "    candidate_df: pd.DataFrame,\n",
        "    filename_col: str = \"filename\",\n",
        "    content_col: Optional[str] = None,\n",
        "    root_dir: Optional[str] = None,\n",
        "    out_dir: str = DEFAULT_OUT_DIR,\n",
        "    num_perm: int = DEFAULT_NUM_PERM,\n",
        "    k_shingle: int = DEFAULT_K_SHINGLE,\n",
        "    lsh_threshold: float = DEFAULT_LSH_THRESHOLD,\n",
        "    top_n_print: int = DEFAULT_TOP_N_PRINT,\n",
        "    save_parquet: bool = True,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Execute MinHash/LSH + exact Jaccard over files referenced in candidate_df.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    candidate_df : DataFrame with at least a 'filename' column of absolute paths.\n",
        "    filename_col : Column name containing absolute file paths (default 'filename').\n",
        "    content_col  : Optional column name containing already-loaded source text.\n",
        "    out_dir      : Output directory for CSV/Parquet/JSONL.\n",
        "    num_perm     : MinHash permutations.\n",
        "    k_shingle    : k for k-shingles.\n",
        "    lsh_threshold: LSH candidate threshold.\n",
        "    top_n_print  : How many top pairs to preview in logs.\n",
        "    save_parquet : Save parquet alongside CSV.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (df_files, df_pairs, similar_files)\n",
        "    \"\"\"\n",
        "    _ensure_outdir(out_dir)\n",
        "    print(f\"[info] ==== Starting MinHash/LSH over DataFrame ====\")\n",
        "    print(f\"[info] params: K_SHINGLE={k_shingle}, NUM_PERM={num_perm}, LSH_THRESHOLD={lsh_threshold}\")\n",
        "\n",
        "    # Build corpus from DataFrame (open UTF-8, normalize newlines)\n",
        "    corpus = _load_corpus_from_df(\n",
        "            candidate_df,\n",
        "            filename_col=filename_col,\n",
        "            content_col=content_col,\n",
        "            root_dir=root_dir,\n",
        "        )\n",
        "    if not corpus:\n",
        "        print(\"[fatal] No readable files from candidate_df; aborting.\")\n",
        "        return (pd.DataFrame(), pd.DataFrame(), [])\n",
        "\n",
        "    print(f\"[info] loaded {len(corpus)} files from candidate_df\")\n",
        "\n",
        "    # Build LSH and per-file signatures\n",
        "    lsh = MinHashLSH(threshold=lsh_threshold, num_perm=num_perm)\n",
        "    file_rows = []\n",
        "    file_sigs: Dict[str, Tuple[set, MinHash]] = {}\n",
        "\n",
        "    for path, text in corpus.items():\n",
        "        toks = tokenize(text)\n",
        "        S = shingles(toks, k=k_shingle)\n",
        "        mh = to_minhash(S, num_perm)\n",
        "        lsh.insert(path, mh)\n",
        "        file_sigs[path] = (S, mh)\n",
        "\n",
        "        hashvalues = mh.hashvalues.astype(np.uint64).tolist()\n",
        "        file_rows.append({\n",
        "            \"filename\": path,\n",
        "            \"num_tokens\": len(toks),\n",
        "            \"num_shingles\": len(S),\n",
        "            \"num_perm\": num_perm,\n",
        "            \"k_shingle\": k_shingle,\n",
        "            \"minhash_hashvalues\": hashvalues,\n",
        "        })\n",
        "\n",
        "    df_files = pd.DataFrame(file_rows).sort_values(\"filename\").reset_index(drop=True)\n",
        "    print(f\"[info] files indexed   : {len(df_files)}\")\n",
        "\n",
        "    # Candidate pairs: LSH + exact Jaccard\n",
        "    pair_rows = []\n",
        "    keys = list(corpus.keys())\n",
        "    similar_files: List[str] = []\n",
        "\n",
        "    for a in keys:\n",
        "        S_a, mh_a = file_sigs[a]\n",
        "        for b in lsh.query(mh_a):\n",
        "            if b <= a:  # avoid (a,a) and dup pair directions by lexicographic order\n",
        "                continue\n",
        "            S_b, _ = file_sigs[b]\n",
        "            s = jaccard(S_a, S_b)\n",
        "            pair_rows.append({\n",
        "                \"a\": a,\n",
        "                \"b\": b,\n",
        "                \"jaccard\": s,\n",
        "                \"a_shingles\": len(S_a),\n",
        "                \"b_shingles\": len(S_b),\n",
        "                \"union_shingles\": len(S_a | S_b),\n",
        "                \"intersect_shingles\": len(S_a & S_b),\n",
        "            })\n",
        "            if a not in similar_files:\n",
        "                similar_files.append(a)\n",
        "            if b not in similar_files:\n",
        "                similar_files.append(b)\n",
        "\n",
        "    df_pairs = pd.DataFrame(pair_rows).sort_values(\"jaccard\", ascending=False).reset_index(drop=True)\n",
        "    keep_t = lsh_threshold\n",
        "    print(f\"[diag] total candidate pairs: {len(df_pairs)}\")\n",
        "    print(f\"[diag] pairs with jaccard >= {keep_t}: {(df_pairs['jaccard'] >= keep_t).sum()}\")\n",
        "\n",
        "    # ---------- save locally (CSV + optional Parquet) ----------\n",
        "    files_csv      = os.path.join(out_dir, \"minhash_files.csv\")\n",
        "    pairs_csv      = os.path.join(out_dir, \"minhash_pairs.csv\")\n",
        "    files_parquet  = os.path.join(out_dir, \"minhash_files.parquet\")\n",
        "    pairs_parquet  = os.path.join(out_dir, \"minhash_pairs.parquet\")\n",
        "    sig_jsonl_path = os.path.join(out_dir, \"minhash_signatures.jsonl\")\n",
        "\n",
        "    df_files.to_csv(files_csv, index=False)\n",
        "    df_pairs.to_csv(pairs_csv, index=False)\n",
        "\n",
        "    if save_parquet:\n",
        "        try:\n",
        "            df_files.to_parquet(files_parquet, index=False)\n",
        "            df_pairs.to_parquet(pairs_parquet, index=False)\n",
        "            print(f\"[info] wrote CSV and Parquet to {out_dir}/\")\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] parquet save failed ({e}); CSVs were still written to {out_dir}/\")\n",
        "\n",
        "    # JSONL of signatures for later reuse\n",
        "    with open(sig_jsonl_path, \"w\", encoding=\"utf-8\") as w:\n",
        "        for _, row in df_files.iterrows():\n",
        "            out = {\n",
        "                \"filename\": row[\"filename\"],\n",
        "                \"num_tokens\": int(row[\"num_tokens\"]),\n",
        "                \"num_shingles\": int(row[\"num_shingles\"]),\n",
        "                \"num_perm\":     int(row[\"num_perm\"]),\n",
        "                \"k_shingle\":    int(row[\"k_shingle\"]),\n",
        "                \"minhash_hashvalues\": row[\"minhash_hashvalues\"],\n",
        "            }\n",
        "            w.write(json.dumps(out) + \"\\n\")\n",
        "\n",
        "    # ---------- diagnostics ----------\n",
        "    print()\n",
        "    print(\"[info] ==== MinHash/LSH run summary ====\")\n",
        "    print(f\"[info] files loaded  : {len(corpus)}\")\n",
        "    print(f\"[info] files indexed : {len(df_files)}\")\n",
        "\n",
        "    zero_shingle = sum(1 for (_, (S, _)) in file_sigs.items() if not S)\n",
        "    print(f\"[info] files with 0 shingles (tokens < {k_shingle}): {zero_shingle}\")\n",
        "\n",
        "    print(f\"[info] candidate pairs (from LSH) : {len(df_pairs)}\")\n",
        "    if not df_pairs.empty:\n",
        "        for c in (0.6, 0.7, 0.8, 0.85, 0.9):\n",
        "            n = int((df_pairs[\"jaccard\"] >= c).sum())\n",
        "            print(f\"[info] pairs with Jaccard >= {c:.2f}: {n}\")\n",
        "        print(f\"[info] avg Jaccard (candidates)  : {df_pairs['jaccard'].mean():.4f}\")\n",
        "        print(f\"[info] max Jaccard               : {df_pairs['jaccard'].max():.4f}\")\n",
        "        print(f\"[info] min Jaccard               : {df_pairs['jaccard'].min():.4f}\")\n",
        "\n",
        "        print(\"\\n[info] top pairs:\")\n",
        "        cols = [\"a\", \"b\", \"jaccard\", \"a_shingles\", \"b_shingles\", \"union_shingles\", \"intersect_shingles\"]\n",
        "        present = [c for c in cols if c in df_pairs.columns]\n",
        "        print(df_pairs.sort_values(\"jaccard\", ascending=False).head(top_n_print)[present].to_string(index=False))\n",
        "    else:\n",
        "        print(\"[warn] LSH returned zero candidates.\")\n",
        "        print(\"       Try K_SHINGLE=3–4, LSH_THRESHOLD=0.6–0.7, NUM_PERM ≥ 256.\")\n",
        "\n",
        "    print(f\"[info] saved hash signatures JSONL: {sig_jsonl_path}\")\n",
        "\n",
        "    return df_files, df_pairs, similar_files\n",
        "\n",
        "# ---------- example usage (not executed on import) ----------\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: expect a parquet or csv with a 'filename' column of absolute paths.\n",
        "    # python similiar_process.py /path/to/candidate_df.parquet\n",
        "    import argparse\n",
        "    p = argparse.ArgumentParser(description=\"Run MinHash/LSH over candidate_df of absolute file paths\")\n",
        "    p.add_argument(\"input\", help=\"candidate_df .parquet or .csv with a 'filename' column\")\n",
        "    p.add_argument(\"--filename-col\", default=\"filename\")\n",
        "    p.add_argument(\"--content-col\", default=None)\n",
        "    p.add_argument(\"--out-dir\", default=DEFAULT_OUT_DIR)\n",
        "    p.add_argument(\"--num-perm\", type=int, default=DEFAULT_NUM_PERM)\n",
        "    p.add_argument(\"--k-shingle\", type=int, default=DEFAULT_K_SHINGLE)\n",
        "    p.add_argument(\"--lsh-threshold\", type=float, default=DEFAULT_LSH_THRESHOLD)\n",
        "    p.add_argument(\"--no-parquet\", action=\"store_true\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    # Load df\n",
        "    if args.input.lower().endswith(\".parquet\"):\n",
        "        df = pd.read_parquet(args.input)\n",
        "    elif args.input.lower().endswith(\".csv\"):\n",
        "        df = pd.read_csv(args.input)\n",
        "    else:\n",
        "        print(\"[fatal] Unsupported input format. Use .parquet or .csv\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    run_from_dataframe(\n",
        "        df,\n",
        "        filename_col=args.filename_col,\n",
        "        content_col=args.content_col if args.content_col else None,\n",
        "        out_dir=args.out_dir,\n",
        "        num_perm=args.num_perm,\n",
        "        k_shingle=args.k_shingle,\n",
        "        lsh_threshold=args.lsh_threshold,\n",
        "        save_parquet=not args.no_parquet,\n",
        "    )"
      ]
    }
  ]
}