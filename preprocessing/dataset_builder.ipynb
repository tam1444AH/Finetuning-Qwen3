{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDz7AaGKIyl7Y2x0YS8WOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/UH-Insure-NSA/blob/main/preprocessing/dataset_builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h67W1rpmcrjf"
      },
      "outputs": [],
      "source": [
        "# dataset_builder.py\n",
        "import json\n",
        "import re\n",
        "import re as _re2\n",
        "from datetime import datetime\n",
        "import math\n",
        "import sys\n",
        "import csv\n",
        "import warnings\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional, Tuple, Callable\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    pd = None\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError as _FutTimeout\n",
        "\n",
        "# persistent decisions (optional)\n",
        "_DECISION_CACHE: Dict[str, bool] = {}\n",
        "_DECISION_CACHE_PATH: Optional[Path] = None\n",
        "_COMMENTISH_STARTS = (\"//\", \"#\", \"--\", \";\", \"/*\", \"*\", \"*/\")\n",
        "\n",
        "def _line_type(line: str) -> str:\n",
        "    s = line.lstrip()\n",
        "    if not s:\n",
        "        return \"blank\"\n",
        "    return \"comment\" if s.startswith(_COMMENTISH_STARTS) else \"code\"\n",
        "\n",
        "def normalize_separation_policy(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Enforce:\n",
        "      1) No leading blank lines.\n",
        "      2) Code↔Code: no empty blank line between (just newline).\n",
        "      3) Any transition with comments: at most one empty line between.\n",
        "      4) Exactly one trailing '\\n' at EOF.\n",
        "    \"\"\"\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    # 1) drop leading blanks\n",
        "    i = 0\n",
        "    n = len(lines)\n",
        "    while i < n and lines[i].strip() == \"\":\n",
        "        i += 1\n",
        "\n",
        "    out: list[str] = []\n",
        "    prev_type: str | None = None\n",
        "\n",
        "    while i < n:\n",
        "        # accumulate blanks\n",
        "        if lines[i].strip() == \"\":\n",
        "            # count run of blanks\n",
        "            j = i\n",
        "            while j < n and lines[j].strip() == \"\":\n",
        "                j += 1\n",
        "            # if blanks go to EOF, stop (no trailing blanks)\n",
        "            if j >= n:\n",
        "                break\n",
        "            next_type = _line_type(lines[j])\n",
        "            # decide how many blank lines to keep between prev non-blank and next non-blank\n",
        "            if prev_type == \"code\" and next_type == \"code\":\n",
        "                keep_blanks = 0  # Rule 2\n",
        "            else:\n",
        "                keep_blanks = 1  # Rule 3 (at most one)\n",
        "            out.extend([\"\"] * keep_blanks)\n",
        "            i = j\n",
        "            continue\n",
        "\n",
        "        # non-blank line\n",
        "        lt = _line_type(lines[i])\n",
        "        out.append(lines[i])\n",
        "        prev_type = lt\n",
        "        i += 1\n",
        "\n",
        "    # 4) exactly one trailing newline\n",
        "    return (\"\\n\".join(out)).rstrip(\"\\n\") + \"\\n\"\n",
        "\n",
        "def _load_decision_cache(path: Optional[Path]):\n",
        "    global _DECISION_CACHE, _DECISION_CACHE_PATH\n",
        "    _DECISION_CACHE_PATH = path\n",
        "    if path and path.exists():\n",
        "        try:\n",
        "            with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    obj = json.loads(line)\n",
        "                    h = obj.get(\"sha1\"); keep = obj.get(\"keep\")\n",
        "                    if isinstance(h, str) and isinstance(keep, bool):\n",
        "                        _DECISION_CACHE[h] = keep\n",
        "            print(f\"[hybrid-agent] loaded {len(_DECISION_CACHE)} cached decisions from {path}\")\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"could not load decision cache: {e}\")\n",
        "\n",
        "def _append_decision_cache(h: str, keep: bool):\n",
        "    global _DECISION_CACHE_PATH\n",
        "    if _DECISION_CACHE_PATH:\n",
        "        try:\n",
        "            with _DECISION_CACHE_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(json.dumps({\"sha1\": h, \"keep\": keep}) + \"\\n\")\n",
        "        except Exception:\n",
        "            pass  # cache write errors are non-fatal\n",
        "\n",
        "def _hash_txt(txt: str) -> str:\n",
        "    return hashlib.sha1(txt.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "def estimate_tokens(text: str, *, chars_per_token: float = 4.0) -> int:\n",
        "    \"\"\"Very rough token estimate; errs high when chars_per_token < true ratio.\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return max(1, math.ceil(len(text) / max(1e-6, chars_per_token)))\n",
        "\n",
        "def split_text_by_token_budget(\n",
        "    text: str,\n",
        "    *,\n",
        "    max_tokens: int,\n",
        "    overlap_tokens: int = 64,\n",
        "    chars_per_token: float = 4.0,\n",
        "    prefer_line_breaks: bool = True\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into chunks that each fit within max_tokens (roughly).\n",
        "    Uses a char-window consistent with chars_per_token, adds small overlap to reduce boundary artifacts.\n",
        "    \"\"\"\n",
        "    if estimate_tokens(text, chars_per_token=chars_per_token) <= max_tokens:\n",
        "        return [text]\n",
        "\n",
        "    # Convert token budgets to char budgets\n",
        "    max_chars = int(max_tokens * chars_per_token)\n",
        "    overlap_chars = int(overlap_tokens * chars_per_token)\n",
        "\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    n = len(text)\n",
        "    while i < n:\n",
        "        end = min(n, i + max_chars)\n",
        "        chunk = text[i:end]\n",
        "\n",
        "        if prefer_line_breaks and end < n:\n",
        "            # try to backtrack to last newline to avoid cutting mid-line\n",
        "            back = chunk.rfind(\"\\n\")\n",
        "            if back >= int(0.5 * max_chars):  # don’t backtrack too far (keeps chunks reasonable)\n",
        "                end = i + back + 1\n",
        "                chunk = text[i:end]\n",
        "\n",
        "        chunks.append(chunk)\n",
        "        if end >= n:\n",
        "            break\n",
        "        i = max(0, end - overlap_chars)\n",
        "    return chunks\n",
        "\n",
        "# ---------- Progress helpers ----------\n",
        "def _print_every(n: int, total: Optional[int], label: str):\n",
        "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    if total:\n",
        "        pct = (n / total) * 100.0\n",
        "        print(f\"[{label}] {now} processed {n}/{total} ({pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"[{label}] {now} processed {n}\")\n",
        "\n",
        "\n",
        "# ---------- Lazy import agent (now supports batching + progress) ----------\n",
        "def _lazy_import_policy():\n",
        "    try:\n",
        "        from preprocessing.comment_policy_agent import decide_keep_drop_batch\n",
        "        return decide_keep_drop_batch\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Hybrid comment agent unavailable ({e}). Falling back to heuristic batch.\")\n",
        "        def heuristic_batch(items: List[Dict]) -> List[bool]:\n",
        "            outs = []\n",
        "            for it in items:\n",
        "                txt = it[\"comment_text\"]\n",
        "                lower = txt.lower()\n",
        "                if any(k in lower for k in [\"copyright\", \"license\", \"warranty\", \"apache\", \"mit\", \"bsd\", \"gnu\"]):\n",
        "                    outs.append(False); continue\n",
        "                if len(txt) > 1000 and \"http\" in lower and \"@\" in txt:\n",
        "                    outs.append(False); continue\n",
        "                if any(k in lower for k in [\"args\", \"parameters\", \"returns\", \"example\", \"usage\", \"invariant\",\n",
        "                                            \"precondition\", \"postcondition\", \"proof\", \"spec\"]):\n",
        "                    outs.append(True); continue\n",
        "                outs.append(len(txt) < 500)\n",
        "            return outs\n",
        "        return heuristic_batch\n",
        "\n",
        "# ---------- Normalization & IO ----------\n",
        "def read_text_normalized(path: Path) -> Optional[str]:\n",
        "    if not path.exists() or not path.is_file():\n",
        "        return None\n",
        "    try:\n",
        "        data = path.read_bytes()\n",
        "        text = data.decode('utf-8', errors='replace')\n",
        "        # normalize newlines (real \\r,\\r\\n -> \\n)\n",
        "        text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "        return text\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def prepend_root(p: Path, root_dir: Optional[Path]) -> Path:\n",
        "    if not root_dir:\n",
        "        return p\n",
        "    try:\n",
        "        # If absolute, leave as-is; else join root\n",
        "        return p if p.is_absolute() else (root_dir / p)\n",
        "    except Exception:\n",
        "        return p\n",
        "\n",
        "# ---------- Language guess ----------\n",
        "EXT_LANG = {\n",
        "    '.py': 'python', '.js': 'javascript', '.ts': 'typescript', '.java': 'java',\n",
        "    '.c': 'c', '.h': 'c', '.hpp': 'cpp', '.cpp': 'cpp', '.cc': 'cpp',\n",
        "    '.go': 'go', '.rs': 'rust', '.rb': 'ruby', '.php': 'php', '.sh': 'shell',\n",
        "    '.bash': 'shell', '.zsh': 'shell', '.ps1': 'powershell', '.sql': 'sql',\n",
        "    '.css': 'css', '.scss': 'css', '.md': 'markdown', '.txt': 'text',\n",
        "    '.yaml': 'yaml', '.yml': 'yaml', '.cry': 'cryptol', '.saw': 'saw',\n",
        "    '.lean': 'lean', '.hs': 'haskell',\n",
        "}\n",
        "def guess_lang_from_ext(path: Path) -> str:\n",
        "    return EXT_LANG.get(path.suffix.lower(), 'text')\n",
        "\n",
        "# ---------- Comment extraction/stripping ----------\n",
        "BLOCK_C_STYLES = [\n",
        "    (re.compile(r'/\\*.*?\\*/', re.S), '/*', '*/'),\n",
        "]\n",
        "\n",
        "LINE_COMMENT_PATTERNS = [\n",
        "    (re.compile(r'(^|\\s)//[^\\n]*', re.M), '//'),\n",
        "    (re.compile(r'(^|\\s)#(?!\\!).*', re.M), '#'),\n",
        "    (re.compile(r'(^|\\s)--[^\\n]*', re.M), '--'),\n",
        "    (re.compile(r'(^|\\s);[^\\n]*', re.M), ';'),\n",
        "]\n",
        "\n",
        "def extract_comments_raw(text: str) -> List[Tuple[str, Tuple[int,int], str]]:\n",
        "    \"\"\"Return list of (comment_text, (start,end), kind) spans (kind in {'block','//','#','--',';'}).\"\"\"\n",
        "    spans = []\n",
        "    for rx, start_tok, end_tok in BLOCK_C_STYLES:\n",
        "        for m in rx.finditer(text):\n",
        "            spans.append((m.group(0), (m.start(), m.end()), 'block'))\n",
        "    for rx, tok in LINE_COMMENT_PATTERNS:\n",
        "        k = tok\n",
        "        for m in rx.finditer(text):\n",
        "            spans.append((m.group(0), (m.start(), m.end()), k))\n",
        "    spans.sort(key=lambda x: x[1][0])\n",
        "    return spans\n",
        "\n",
        "def group_consecutive_slashslash(text: str, spans: List[Tuple[str, Tuple[int,int], str]]) -> List[Tuple[str, Tuple[int,int], str]]:\n",
        "    \"\"\"Group consecutive '//' line comments touching line-by-line into a single multi-line comment span.\"\"\"\n",
        "    grouped = []\n",
        "    i = 0\n",
        "    while i < len(spans):\n",
        "        txt, (s, e), kind = spans[i]\n",
        "        if kind != '//':\n",
        "            grouped.append((txt, (s, e), kind))\n",
        "            i += 1\n",
        "            continue\n",
        "        # Start a group of //\n",
        "        start, end = s, e\n",
        "        chunk = [txt]\n",
        "        i += 1\n",
        "        while i < len(spans) and spans[i][2] == '//':\n",
        "            s2, e2 = spans[i][1]\n",
        "            inter = text[end:s2]\n",
        "            if set(inter) <= set([' ', '\\t', '\\n']):  # no code between\n",
        "                end = e2\n",
        "                chunk.append(spans[i][0])\n",
        "                i += 1\n",
        "            else:\n",
        "                break\n",
        "        grouped_txt = ''.join(chunk)\n",
        "        grouped.append((grouped_txt, (start, end), '//'))\n",
        "    grouped.sort(key=lambda x: x[1][0])\n",
        "    return grouped\n",
        "\n",
        "def extract_comments(text: str) -> List[Tuple[str, Tuple[int,int], str]]:\n",
        "    raw = extract_comments_raw(text)\n",
        "    return group_consecutive_slashslash(text, raw)\n",
        "\n",
        "def strip_comments(text: str) -> Tuple[str, List[Tuple[str, Tuple[int,int], str]]]:\n",
        "    \"\"\"Remove comments and return (code_without_comments, list_of_removed (text, (s,e), kind)).\"\"\"\n",
        "    spans = extract_comments(text)\n",
        "    if not spans:\n",
        "        return text, []\n",
        "    out = []\n",
        "    removed = []\n",
        "    idx = 0\n",
        "    for ctext, (s, e), kind in spans:\n",
        "        if s > idx:\n",
        "            out.append(text[idx:s])\n",
        "        removed.append((ctext, (s, e), kind))\n",
        "        newlines = ctext.count('\\n')\n",
        "        out.append('\\n' * newlines)\n",
        "        idx = e\n",
        "    out.append(text[idx:])\n",
        "    return ''.join(out), removed\n",
        "\n",
        "# ---------- Metrics ----------\n",
        "import re as _re\n",
        "HEXBYTE_RE = _re.compile(r'\\b0x[0-9a-fA-F]+\\b')\n",
        "HEXNUM_RE = _re.compile(r'\\b[0-9a-fA-F]{8,}\\b')\n",
        "\n",
        "def compute_basic_metrics(text: str) -> Dict[str, float]:\n",
        "    lines = text.split('\\n')\n",
        "    n_lines = len(lines)\n",
        "    n_bytes = len(text.encode('utf-8', errors='ignore'))\n",
        "    avg_line_len = (sum(len(l) for l in lines) / max(1, n_lines))\n",
        "    max_line_len = max((len(l) for l in lines), default=0)\n",
        "    non_ascii_ratio = sum(1 for ch in text if ord(ch) > 127) / max(1, len(text))\n",
        "    binary_like = int('\\x00' in text)\n",
        "    enc_base64_hits = len(_re.findall(r'\\b[A-Za-z0-9+/=]{24,}\\b', text))\n",
        "    enc_hexbytes_hits = len(HEXBYTE_RE.findall(text))\n",
        "    enc_unicode_hits = len(_re.findall(r'\\\\u[0-9a-fA-F]{4}', text))\n",
        "    enc_total_matched = enc_base64_hits + enc_hexbytes_hits + enc_unicode_hits\n",
        "    enc_max_run = max([len(m.group(0)) for m in _re.finditer(r'[A-Za-z0-9+/=]{1,}', text)] or [0])\n",
        "    enc_fraction = enc_total_matched / max(1, len(text))\n",
        "    lang_tokens = _re.findall(r'[A-Za-z_][A-Za-z0-9_]*', text)\n",
        "    num_tokens_lang = len(lang_tokens)\n",
        "    hexnum_ratio = len(HEXNUM_RE.findall(text)) / max(1, num_tokens_lang)\n",
        "    num_tokens_model = len(_re.findall(r'\\S+', text))\n",
        "    # simple character-shingle count as proxy\n",
        "    k = 5\n",
        "    shingles = set()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        for i in range(max(0, len(line) - k + 1)):\n",
        "            shingles.add(line[i:i+k])\n",
        "    num_shingles = len(shingles)\n",
        "    return {\n",
        "        'bytes': n_bytes, 'lines': n_lines, 'avg_line_len': avg_line_len, 'max_line_len': max_line_len,\n",
        "        'non_ascii_ratio': non_ascii_ratio, 'binary_like': binary_like,\n",
        "        'enc_total_matched': enc_total_matched, 'enc_max_run': enc_max_run, 'enc_fraction': enc_fraction,\n",
        "        'enc_hits_base64': enc_base64_hits, 'enc_hits_hexbytes': enc_hexbytes_hits, 'enc_hits_unicode': enc_unicode_hits,\n",
        "        'num_tokens_lang': num_tokens_lang, 'k_shingle': 5, 'num_shingles': num_shingles, 'hexnum_ratio': hexnum_ratio,\n",
        "        'num_tokens_model': num_tokens_model,\n",
        "    }\n",
        "\n",
        "# ---------- Comment hashing & index ----------\n",
        "def hash_comment(txt: str) -> str:\n",
        "    return hashlib.sha1(txt.encode('utf-8', errors='ignore')).hexdigest()\n",
        "\n",
        "_DECISION_CACHE: Dict[str, bool] = {}\n",
        "\n",
        "def decide_batch(\n",
        "    file_path: str,\n",
        "    code_no_comments: str,\n",
        "    spans: List[Tuple[str, Tuple[int,int], str]],\n",
        "    *,\n",
        "    batch_size: int = 10,\n",
        "    progress_cb: Optional[Callable[[int, int], None]] = None,\n",
        "    agent_timeout_s: int = 60,\n",
        "    max_comment_len: int = 4000,\n",
        ") -> List[bool]:\n",
        "    \"\"\"\n",
        "    Decide keep/drop using comment_policy_agent.decide_keep_drop_batch with:\n",
        "      - per-batch timeouts\n",
        "      - one retry with halved batch size\n",
        "      - fallback heuristic if agent still fails\n",
        "      - text truncation to avoid pathological inputs\n",
        "      - persistent cache to skip repeat work\n",
        "    \"\"\"\n",
        "    agent = _lazy_import_policy()\n",
        "\n",
        "    # Prebuild undecided items, honoring cache and truncation\n",
        "    undecided = []\n",
        "    decisions: List[Optional[bool]] = [None]*len(spans)  # type: ignore\n",
        "    for idx, (ctext, (s, e), kind) in enumerate(spans):\n",
        "        h = _hash_txt(ctext)\n",
        "        if h in _DECISION_CACHE:\n",
        "            decisions[idx] = _DECISION_CACHE[h]\n",
        "        else:\n",
        "            # truncate long comments *only for agent input*; decision is still stored by hash of full text\n",
        "            csend = ctext if len(ctext) <= max_comment_len else (ctext[:max_comment_len] + \"\\n/*...truncated...*/\")\n",
        "            undecided.append({\"i\": idx, \"comment_text\": csend, \"file_path\": file_path, \"code_context\": code_no_comments, \"full_hash\": h})\n",
        "\n",
        "    processed = 0\n",
        "    total = len(undecided)\n",
        "\n",
        "    def _run_agent(items):\n",
        "        # run agent in a thread so we can enforce a timeout\n",
        "        with ThreadPoolExecutor(max_workers=1) as ex:\n",
        "            fut = ex.submit(agent, items)\n",
        "            return fut.result(timeout=agent_timeout_s)\n",
        "\n",
        "    i = 0\n",
        "    current_batch = batch_size if batch_size > 0 else 1\n",
        "    while i < total:\n",
        "        batch = undecided[i:i+current_batch]\n",
        "        try:\n",
        "            judged = _run_agent(batch)\n",
        "            if len(judged) != len(batch):\n",
        "                warnings.warn(f\"Agent returned {len(judged)} decisions for batch of {len(batch)}. Aligning by min length.\")\n",
        "            upto = min(len(judged), len(batch))\n",
        "            for j in range(upto):\n",
        "                idx_global = batch[j][\"i\"]\n",
        "                keep = bool(judged[j])\n",
        "                decisions[idx_global] = keep\n",
        "                _DECISION_CACHE[batch[j][\"full_hash\"]] = keep\n",
        "                _append_decision_cache(batch[j][\"full_hash\"], keep)\n",
        "            processed += upto\n",
        "            if progress_cb:\n",
        "                progress_cb(processed, total)\n",
        "            i += current_batch\n",
        "            # if successful with reduced size earlier, we can try to step back up a bit\n",
        "            if current_batch < batch_size:\n",
        "                current_batch = min(batch_size, current_batch * 2)\n",
        "        except _FutTimeout:\n",
        "            warnings.warn(f\"[hybrid-agent] timeout after {agent_timeout_s}s on {file_path} batch starting at undecided[{i}] size={current_batch}. Retrying with half batch.\")\n",
        "            # one retry with half batch size\n",
        "            if current_batch > 1:\n",
        "                current_batch = max(1, current_batch // 2)\n",
        "                continue\n",
        "            # fallback heuristic for single item\n",
        "            idx_global = batch[0][\"i\"]\n",
        "            keep = len(spans[idx_global][0]) < 500\n",
        "            decisions[idx_global] = keep\n",
        "            _DECISION_CACHE[batch[0][\"full_hash\"]] = keep\n",
        "            _append_decision_cache(batch[0][\"full_hash\"], keep)\n",
        "            processed += 1\n",
        "            if progress_cb:\n",
        "                progress_cb(processed, total)\n",
        "            i += 1\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"[hybrid-agent] error on {file_path} batch starting at {i}: {e}. Falling back to heuristic for this batch.\")\n",
        "            # heuristic fallback for this batch\n",
        "            upto = len(batch)\n",
        "            for j in range(upto):\n",
        "                idx_global = batch[j][\"i\"]\n",
        "                keep = len(spans[idx_global][0]) < 500\n",
        "                decisions[idx_global] = keep\n",
        "                _DECISION_CACHE[batch[j][\"full_hash\"]] = keep\n",
        "                _append_decision_cache(batch[j][\"full_hash\"], keep)\n",
        "            processed += upto\n",
        "            if progress_cb:\n",
        "                progress_cb(processed, total)\n",
        "            i += current_batch\n",
        "\n",
        "    # fill any leftover None (belt-and-suspenders)\n",
        "    for k in range(len(decisions)):\n",
        "        if decisions[k] is None:\n",
        "            decisions[k] = len(spans[k][0]) < 500\n",
        "\n",
        "    return [bool(x) for x in decisions]  # type: ignore\n",
        "\n",
        "\n",
        "def apply_hybrid_policy(\n",
        "    original_text: str,\n",
        "    file_path: str,\n",
        "    code_no_comments: str,\n",
        "    comments_index_fh,\n",
        "    *,\n",
        "    batch_size: int = 10,\n",
        "    show_progress: bool = True,\n",
        "    agent_timeout_s: int = 60,\n",
        "    max_comment_len: int = 4000,\n",
        ") -> str:\n",
        "    spans = extract_comments(original_text)\n",
        "    if not spans:\n",
        "        return original_text\n",
        "\n",
        "    def _cb(done: int, total: int):\n",
        "        if show_progress:\n",
        "            print(f\"[hybrid-agent] {file_path}: decided {done}/{total} comment chunks\")\n",
        "\n",
        "    keeps = decide_batch(\n",
        "        file_path,\n",
        "        code_no_comments,\n",
        "        spans,\n",
        "        batch_size=batch_size,\n",
        "        progress_cb=_cb if show_progress else None,\n",
        "        agent_timeout_s=agent_timeout_s,\n",
        "        max_comment_len=max_comment_len,\n",
        "    )\n",
        "\n",
        "    out = []\n",
        "    idx = 0\n",
        "    for (ctext, (s, e), kind), keep in zip(spans, keeps):\n",
        "        if s > idx:\n",
        "            out.append(original_text[idx:s])\n",
        "        if keep:\n",
        "            out.append(ctext)\n",
        "        else:\n",
        "            out.append('\\n' * ctext.count('\\n'))\n",
        "        rec = {\n",
        "            \"filename\": file_path, \"span_start\": s, \"span_end\": e, \"kind\": kind,\n",
        "            \"comment_sha1\": hash_comment(ctext), \"kept\": bool(keep), \"length\": len(ctext)\n",
        "        }\n",
        "        comments_index_fh.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "        idx = e\n",
        "    out.append(original_text[idx:])\n",
        "    return ''.join(out)\n",
        "\n",
        "def build_records_for_file(\n",
        "    file_path: Path,\n",
        "    variant: str,\n",
        "    comments_index_fh=None,\n",
        "    *,\n",
        "    agent_batch_size: int = 10,\n",
        "    show_agent_progress: bool = True,\n",
        "    agent_timeout_s: int = 60,\n",
        "    max_comment_len: int = 4000,\n",
        "    # NEW: model context controls\n",
        "    context_window_tokens: int = 4096,\n",
        "    prompt_reserve_tokens: int = 512,\n",
        "    chunk_overlap_tokens: int = 64,\n",
        "    chars_per_token: float = 4.0,\n",
        ") -> Optional[List[Dict]]:\n",
        "    \"\"\"\n",
        "    Returns a LIST of records (one per chunk) for this file+variant,\n",
        "    or None if file unreadable.\n",
        "    \"\"\"\n",
        "    text = read_text_normalized(file_path)\n",
        "    if text is None:\n",
        "        return None\n",
        "    text = text.strip(\"\\n\") + \"\\n\"\n",
        "    lang = guess_lang_from_ext(file_path)\n",
        "    code_wo, removed = strip_comments(text)\n",
        "\n",
        "    if variant == 'with_comments':\n",
        "        content = text\n",
        "    elif variant == 'without_comments':\n",
        "        content = code_wo\n",
        "    elif variant == 'hybrid':\n",
        "        if comments_index_fh is None:\n",
        "            raise ValueError(\"comments_index_fh is required for hybrid variant\")\n",
        "        content = apply_hybrid_policy(\n",
        "            text, str(file_path), code_wo, comments_index_fh,\n",
        "            batch_size=agent_batch_size, show_progress=show_agent_progress,\n",
        "            agent_timeout_s=agent_timeout_s, max_comment_len=max_comment_len\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown variant: {variant}\")\n",
        "\n",
        "    content = normalize_separation_policy(content)\n",
        "    # --- chunk to fit model context (accounting for prompt tokens) ---\n",
        "    max_usable_tokens = max(1, context_window_tokens - prompt_reserve_tokens)\n",
        "    chunks = split_text_by_token_budget(\n",
        "        content,\n",
        "        max_tokens=max_usable_tokens,\n",
        "        overlap_tokens=chunk_overlap_tokens,\n",
        "        chars_per_token=chars_per_token,\n",
        "        prefer_line_breaks=True\n",
        "    )\n",
        "\n",
        "    records: List[Dict] = []\n",
        "    total_parts = len(chunks)\n",
        "    for idx, chunk in enumerate(chunks, start=1):\n",
        "        metrics = compute_basic_metrics(chunk)\n",
        "        sha1 = hashlib.sha1(chunk.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "        rec = {\n",
        "            'filename': str(file_path),\n",
        "            'lang': lang,\n",
        "            'variant': variant,\n",
        "            'content': chunk,\n",
        "            'sha1': sha1,\n",
        "            'nchars': len(chunk),\n",
        "            # chunking metadata (handy for training/sampling)\n",
        "            'chunk_idx': idx,\n",
        "            'chunks_total': total_parts,\n",
        "            'context_window_tokens': context_window_tokens,\n",
        "            'prompt_reserve_tokens': prompt_reserve_tokens,\n",
        "            **metrics,\n",
        "        }\n",
        "        records.append(rec)\n",
        "\n",
        "    return records\n",
        "\n",
        "\n",
        "def iter_source_files_from_metrics_csv(csv_path: Path, filename_col: str = 'filename',\n",
        "                                       *, root_dir: Optional[Path] = None) -> Iterable[Path]:\n",
        "    with csv_path.open('r', newline='') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if filename_col in row and row[filename_col]:\n",
        "                yield prepend_root(Path(row[filename_col]), root_dir)\n",
        "\n",
        "def iter_source_files_from_jsonl(jsonl_path: Path, filename_field: str = 'filename',\n",
        "                                 *, root_dir: Optional[Path] = None) -> Iterable[Path]:\n",
        "    with jsonl_path.open('r') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                if filename_field in obj and obj[filename_field]:\n",
        "                    yield prepend_root(Path(obj[filename_field]), root_dir)\n",
        "            except Exception:\n",
        "                continue\n",
        "def build_datasets(\n",
        "    inputs: List[Path], out_dir: Path,\n",
        "    variants: List[str] = ('with_comments','without_comments','hybrid'),\n",
        "    save_jsonl: bool = True, save_parquet: bool = True,\n",
        "    *, agent_batch_size: int = 10, show_agent_progress: bool = True,\n",
        "    file_progress_every: int = 25,\n",
        "    agent_timeout_s: int = 60, max_comment_len: int = 4000,\n",
        "    decision_cache_path: Optional[Path] = None,\n",
        "    # model context controls\n",
        "    context_window_tokens: int = 4096,\n",
        "    prompt_reserve_tokens: int = 512,\n",
        "    chunk_overlap_tokens: int = 64,\n",
        "    chars_per_token: float = 4.0,\n",
        ") -> Dict[str, List[Dict]]:\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # 1) load persistent agent decisions (safe no-op if path is None/missing)\n",
        "    _load_decision_cache(decision_cache_path)\n",
        "\n",
        "    results: Dict[str, List[Dict]] = {v: [] for v in variants}\n",
        "    comments_index_fh = None\n",
        "    if 'hybrid' in variants:\n",
        "        comments_index_fh = (out_dir / 'comments_index.jsonl').open('w', encoding='utf-8')\n",
        "\n",
        "    total_files = len(inputs)\n",
        "    for idx_file, p in enumerate(inputs, start=1):\n",
        "        # pretty, time-stamped progress line\n",
        "        if file_progress_every and (idx_file % file_progress_every == 0 or idx_file == total_files):\n",
        "            _print_every(idx_file, total_files, \"dataset-builder\")\n",
        "\n",
        "        for v in variants:\n",
        "            recs = build_records_for_file(\n",
        "                p, v, comments_index_fh=comments_index_fh,\n",
        "                agent_batch_size=agent_batch_size,\n",
        "                show_agent_progress=show_agent_progress,\n",
        "                agent_timeout_s=agent_timeout_s,\n",
        "                max_comment_len=max_comment_len,\n",
        "                context_window_tokens=context_window_tokens,\n",
        "                prompt_reserve_tokens=prompt_reserve_tokens,\n",
        "                chunk_overlap_tokens=chunk_overlap_tokens,\n",
        "                chars_per_token=chars_per_token,\n",
        "            )\n",
        "            if recs:\n",
        "                # 2) extend because we get a list of chunked records per file/variant\n",
        "                results[v].extend(recs)\n",
        "\n",
        "    if comments_index_fh is not None:\n",
        "        comments_index_fh.close()\n",
        "\n",
        "    # Always write the three JSONLs for fine-tuning (even if variants was narrowed)\n",
        "    if save_jsonl:\n",
        "        for v in ('with_comments','without_comments','hybrid'):\n",
        "            if v not in results:\n",
        "                results[v] = []\n",
        "            jpath = out_dir / f'dataset_{v}.jsonl'\n",
        "            with jpath.open('w', encoding='utf-8') as f:\n",
        "                for r in results[v]:\n",
        "                    f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    if save_parquet and pd is not None:\n",
        "        for v, recs in results.items():\n",
        "            dpath = out_dir / f'dataset_{v}.parquet'\n",
        "            df = pd.DataFrame(recs)\n",
        "            df.to_parquet(dpath, index=False)\n",
        "    elif save_parquet and pd is None:\n",
        "        warnings.warn(\"pandas not installed — skipping parquet output.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------- Notebook-friendly wrapper ----------\n",
        "def build_datasets_from_sources(\n",
        "    *,\n",
        "    metrics_csv: Optional[str] = None,\n",
        "    jsonl: Optional[str] = None,\n",
        "    filename_col: str = 'filename',\n",
        "    filename_field: str = 'filename',\n",
        "    out_dir: str = 'out_datasets',\n",
        "    variants: str = 'with_comments,without_comments,hybrid',\n",
        "    root_dir: Optional[str] = None,\n",
        "    agent_batch_size: int = 10,\n",
        "    show_agent_progress: bool = True,\n",
        "    file_progress_every: int = 25,\n",
        "    save_parquet: bool = True,\n",
        "    agent_timeout_s: int = 60,\n",
        "    max_comment_len: int = 4000,\n",
        "    decision_cache_path: Optional[str] = None,\n",
        "    # NEW: model context controls\n",
        "    context_window_tokens: int = 4096,\n",
        "    prompt_reserve_tokens: int = 512,\n",
        "    chunk_overlap_tokens: int = 64,\n",
        "    chars_per_token: float = 4.0,\n",
        ") -> Dict[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    High-level API for notebooks: pass metrics and/or jsonl, optional root_dir.\n",
        "    \"\"\"\n",
        "    inputs: List[Path] = []\n",
        "    root = Path(root_dir) if root_dir else None\n",
        "\n",
        "    if metrics_csv:\n",
        "        inputs.extend(iter_source_files_from_metrics_csv(Path(metrics_csv), filename_col, root_dir=root))\n",
        "    if jsonl:\n",
        "        inputs.extend(iter_source_files_from_jsonl(Path(jsonl), filename_field, root_dir=root))\n",
        "\n",
        "    # unique inputs (preserve order)\n",
        "    seen = set(); unique_inputs = []\n",
        "    for p in inputs:\n",
        "        if p not in seen:\n",
        "            seen.add(p); unique_inputs.append(p)\n",
        "\n",
        "    vlist = [v.strip() for v in variants.split(',') if v.strip()]\n",
        "    return build_datasets(\n",
        "        unique_inputs,\n",
        "        Path(out_dir),\n",
        "        variants=vlist,\n",
        "        save_jsonl=True,\n",
        "        save_parquet=save_parquet,\n",
        "        agent_batch_size=agent_batch_size,\n",
        "        show_agent_progress=show_agent_progress,\n",
        "        file_progress_every=file_progress_every,\n",
        "        agent_timeout_s=agent_timeout_s,\n",
        "        max_comment_len=max_comment_len,\n",
        "        decision_cache_path=Path(decision_cache_path) if decision_cache_path else None,\n",
        "        context_window_tokens=context_window_tokens,\n",
        "        prompt_reserve_tokens=prompt_reserve_tokens,\n",
        "        chunk_overlap_tokens=chunk_overlap_tokens,\n",
        "        chars_per_token=chars_per_token,\n",
        "    )\n",
        "\n",
        "# ---------- CLI ----------\n",
        "def main():\n",
        "    import argparse\n",
        "    ap = argparse.ArgumentParser(description=\"Build 3 datasets: with comments, without comments, and hybrid (agent, batched, // grouped)\")\n",
        "    ap.add_argument('--metrics_csv', type=str, default='', help='CSV with a filename column (default: filename)')\n",
        "    ap.add_argument('--jsonl', type=str, default='', help='JSONL with a filename field (default: filename)')\n",
        "    ap.add_argument('--filename_col', type=str, default='filename')\n",
        "    ap.add_argument('--filename_field', type=str, default='filename')\n",
        "    ap.add_argument('--out_dir', type=str, default='out_datasets')\n",
        "    ap.add_argument('--variants', type=str, default='with_comments,without_comments,hybrid')\n",
        "    ap.add_argument('--root_dir', type=str, default='', help='Prepend this root to relative filenames when reading files')\n",
        "    ap.add_argument('--agent_batch_size', type=int, default=10)\n",
        "    ap.add_argument('--no_agent_progress', action='store_true')\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    root = args.root_dir if args.root_dir else None\n",
        "    results = build_datasets_from_sources(\n",
        "        metrics_csv=args.metrics_csv or None,\n",
        "        jsonl=args.jsonl or None,\n",
        "        filename_col=args.filename_col,\n",
        "        filename_field=args.filename_field,\n",
        "        out_dir=args.out_dir,\n",
        "        variants=args.variants,\n",
        "        root_dir=root,\n",
        "        agent_batch_size=args.agent_batch_size,\n",
        "        show_agent_progress=not args.no_agent_progress,\n",
        "    )\n",
        "    print({k: len(v) for k, v in results.items()})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}